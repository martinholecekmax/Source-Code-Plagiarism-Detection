{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder Loading csv dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "TX9Vh2ojtizS"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6g3LKQGwIys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# !mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZDD1Jrqfwxb",
        "colab_type": "code",
        "outputId": "dd9b943d-200e-45a1-d0b3-053f92e1bbc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dropout, UpSampling1D, BatchNormalization\n",
        "from keras.models import Model, load_model\n",
        "from keras.utils import plot_model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # Plot statistical data such as Annotated heatmaps\n",
        "import math\n",
        "\n",
        "import time, datetime"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOx6GMShzCeM",
        "colab_type": "code",
        "outputId": "0c8e57b5-d740-46d4-87f3-41a07b6e1022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "INPUT_SIZE = 200\n",
        "\n",
        "TENSORBOARD_LOGS_PATH = '/content/drive/colab/logs'\n",
        "\n",
        "# MAIN_FOLDER = ('/content/drive/colab/LSTM/{}/').format(datetime.datetime.now().isoformat())\n",
        "# MAIN_FOLDER = ('/content/drive/colab/CNN/{}/').format(datetime.datetime.now().isoformat())\n",
        "MAIN_FOLDER = ('/content/drive/colab/AUTOENCODER/{}/').format(datetime.datetime.now().isoformat())\n",
        "\n",
        "ENCODER_MODEL_PATH = '/content/drive/colab/models/encoder_model2.h5'\n",
        "CHECK_POINT_PATH = 'weights.best.hdf5'\n",
        "\n",
        "# DATASET_PATH = \"/content/drive/colab/Datasets/dataset1.csv\"\n",
        "# DATASET_PATH = \"/content/drive/colab/Datasets/dataset2.csv\"\n",
        "DATASET_PATH = \"/content/drive/colab/Datasets/dataset3.csv\"\n",
        "\n",
        "optimizer = 'adam'\n",
        "loss = 'binary_crossentropy'\n",
        "activation = 'relu'\n",
        "batch_size = 64\n",
        "epochs = 200\n",
        "dropout = 0.2\n",
        "\n",
        "os.mkdir(MAIN_FOLDER)\n",
        "print(MAIN_FOLDER)\n",
        "\n",
        "main_information = list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1aYInB4gOCT",
        "colab_type": "text"
      },
      "source": [
        "# Create Tokenizer and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3kxueT1gMOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create tokenizer\n",
        "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAiVIFC3gZiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_alphabet():\n",
        "    ''' Create alphabet from ASCII character '''\n",
        "    char_dict = {}\n",
        "    for num in range(127):\n",
        "        char_dict[chr(num)] = num + 1\n",
        "    return char_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSuWV2CvgdFy",
        "colab_type": "code",
        "outputId": "effb4d75-137e-4ba9-f172-416353695974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Create Alphabet Vocabulary (Dictonary)\n",
        "char_dict = get_alphabet()\n",
        "print(char_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\x00': 1, '\\x01': 2, '\\x02': 3, '\\x03': 4, '\\x04': 5, '\\x05': 6, '\\x06': 7, '\\x07': 8, '\\x08': 9, '\\t': 10, '\\n': 11, '\\x0b': 12, '\\x0c': 13, '\\r': 14, '\\x0e': 15, '\\x0f': 16, '\\x10': 17, '\\x11': 18, '\\x12': 19, '\\x13': 20, '\\x14': 21, '\\x15': 22, '\\x16': 23, '\\x17': 24, '\\x18': 25, '\\x19': 26, '\\x1a': 27, '\\x1b': 28, '\\x1c': 29, '\\x1d': 30, '\\x1e': 31, '\\x1f': 32, ' ': 33, '!': 34, '\"': 35, '#': 36, '$': 37, '%': 38, '&': 39, \"'\": 40, '(': 41, ')': 42, '*': 43, '+': 44, ',': 45, '-': 46, '.': 47, '/': 48, '0': 49, '1': 50, '2': 51, '3': 52, '4': 53, '5': 54, '6': 55, '7': 56, '8': 57, '9': 58, ':': 59, ';': 60, '<': 61, '=': 62, '>': 63, '?': 64, '@': 65, 'A': 66, 'B': 67, 'C': 68, 'D': 69, 'E': 70, 'F': 71, 'G': 72, 'H': 73, 'I': 74, 'J': 75, 'K': 76, 'L': 77, 'M': 78, 'N': 79, 'O': 80, 'P': 81, 'Q': 82, 'R': 83, 'S': 84, 'T': 85, 'U': 86, 'V': 87, 'W': 88, 'X': 89, 'Y': 90, 'Z': 91, '[': 92, '\\\\': 93, ']': 94, '^': 95, '_': 96, '`': 97, 'a': 98, 'b': 99, 'c': 100, 'd': 101, 'e': 102, 'f': 103, 'g': 104, 'h': 105, 'i': 106, 'j': 107, 'k': 108, 'l': 109, 'm': 110, 'n': 111, 'o': 112, 'p': 113, 'q': 114, 'r': 115, 's': 116, 't': 117, 'u': 118, 'v': 119, 'w': 120, 'x': 121, 'y': 122, 'z': 123, '{': 124, '|': 125, '}': 126, '~': 127}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlbnmkkMgfPU",
        "colab_type": "code",
        "outputId": "c3785ff8-07e9-4b50-b4fe-3c738b4a277a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Create vocabulary and Add it into the tokenizer\n",
        "tk.word_index = char_dict.copy()\n",
        "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
        "print(tk.word_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\x00': 1, '\\x01': 2, '\\x02': 3, '\\x03': 4, '\\x04': 5, '\\x05': 6, '\\x06': 7, '\\x07': 8, '\\x08': 9, '\\t': 10, '\\n': 11, '\\x0b': 12, '\\x0c': 13, '\\r': 14, '\\x0e': 15, '\\x0f': 16, '\\x10': 17, '\\x11': 18, '\\x12': 19, '\\x13': 20, '\\x14': 21, '\\x15': 22, '\\x16': 23, '\\x17': 24, '\\x18': 25, '\\x19': 26, '\\x1a': 27, '\\x1b': 28, '\\x1c': 29, '\\x1d': 30, '\\x1e': 31, '\\x1f': 32, ' ': 33, '!': 34, '\"': 35, '#': 36, '$': 37, '%': 38, '&': 39, \"'\": 40, '(': 41, ')': 42, '*': 43, '+': 44, ',': 45, '-': 46, '.': 47, '/': 48, '0': 49, '1': 50, '2': 51, '3': 52, '4': 53, '5': 54, '6': 55, '7': 56, '8': 57, '9': 58, ':': 59, ';': 60, '<': 61, '=': 62, '>': 63, '?': 64, '@': 65, 'A': 66, 'B': 67, 'C': 68, 'D': 69, 'E': 70, 'F': 71, 'G': 72, 'H': 73, 'I': 74, 'J': 75, 'K': 76, 'L': 77, 'M': 78, 'N': 79, 'O': 80, 'P': 81, 'Q': 82, 'R': 83, 'S': 84, 'T': 85, 'U': 86, 'V': 87, 'W': 88, 'X': 89, 'Y': 90, 'Z': 91, '[': 92, '\\\\': 93, ']': 94, '^': 95, '_': 96, '`': 97, 'a': 98, 'b': 99, 'c': 100, 'd': 101, 'e': 102, 'f': 103, 'g': 104, 'h': 105, 'i': 106, 'j': 107, 'k': 108, 'l': 109, 'm': 110, 'n': 111, 'o': 112, 'p': 113, 'q': 114, 'r': 115, 's': 116, 't': 117, 'u': 118, 'v': 119, 'w': 120, 'x': 121, 'y': 122, 'z': 123, '{': 124, '|': 125, '}': 126, '~': 127, 'UNK': 128}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txyab6teghL1",
        "colab_type": "code",
        "outputId": "c295a62a-4529-42c7-fa19-1fc39d0a8d69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_size = len(tk.word_index)\n",
        "print(vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBwMHfQMgqcR",
        "colab_type": "text"
      },
      "source": [
        "# Loading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z01QKiSzwX8-",
        "colab_type": "code",
        "outputId": "d460158d-4c82-4f07-e2bd-b80f947e739d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "dataset_df = pd.read_csv(DATASET_PATH, header=None)\n",
        "dataset_df.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10126 entries, 0 to 10125\n",
            "Data columns (total 3 columns):\n",
            "0    10126 non-null int64\n",
            "1    10126 non-null object\n",
            "2    10126 non-null object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 237.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu_FRw35yVgA",
        "colab_type": "code",
        "outputId": "3e0568e5-a92c-483d-b0a7-55ca92cb1d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "dataset_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>PelopsException pelopsException = translator....</td>\n",
              "      <td>PelopsException pelopsException = translator....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>ArrayList&lt;Context&gt; javaList = new ArrayList&lt;C...</td>\n",
              "      <td>ArrayLXXXXXst&lt;Context&gt; javaLXXXXXst = new Arr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>ResilientPropagation rprop = new ResilientPro...</td>\n",
              "      <td>ResilientPropagation XXXXX = new ResilientPro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>new DateTimeService().setCurrentDateTime(star...</td>\n",
              "      <td>if ((mask &amp; 1) != 0) count1++; if ((mask &amp; 2)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Thread thread = Thread.currentThread(); Threa...</td>\n",
              "      <td>Parameters p = object.getClass().getAnnotatio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   0                                                  1  \\\n",
              "0  1   PelopsException pelopsException = translator....   \n",
              "1  1   ArrayList<Context> javaList = new ArrayList<C...   \n",
              "2  1   ResilientPropagation rprop = new ResilientPro...   \n",
              "3  0   new DateTimeService().setCurrentDateTime(star...   \n",
              "4  0   Thread thread = Thread.currentThread(); Threa...   \n",
              "\n",
              "                                                   2  \n",
              "0   PelopsException pelopsException = translator....  \n",
              "1   ArrayLXXXXXst<Context> javaLXXXXXst = new Arr...  \n",
              "2   ResilientPropagation XXXXX = new ResilientPro...  \n",
              "3   if ((mask & 1) != 0) count1++; if ((mask & 2)...  \n",
              "4   Parameters p = object.getClass().getAnnotatio...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8Ej3CGIx-yO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_df = dataset_df.drop([0], axis=1)\n",
        "dataset_df = dataset_df.drop([2], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o05NCeDJyk_M",
        "colab_type": "code",
        "outputId": "2a7c7161-956c-4dca-a07e-e8bbc3eeae88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "dataset_df.info()\n",
        "dataset_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10126 entries, 0 to 10125\n",
            "Data columns (total 1 columns):\n",
            "1    10126 non-null object\n",
            "dtypes: object(1)\n",
            "memory usage: 79.2+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PelopsException pelopsException = translator....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ArrayList&lt;Context&gt; javaList = new ArrayList&lt;C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ResilientPropagation rprop = new ResilientPro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>new DateTimeService().setCurrentDateTime(star...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thread thread = Thread.currentThread(); Threa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   1\n",
              "0   PelopsException pelopsException = translator....\n",
              "1   ArrayList<Context> javaList = new ArrayList<C...\n",
              "2   ResilientPropagation rprop = new ResilientPro...\n",
              "3   new DateTimeService().setCurrentDateTime(star...\n",
              "4   Thread thread = Thread.currentThread(); Threa..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4u1LlS6xmt6",
        "colab_type": "code",
        "outputId": "93cfc836-9086-4b79-9eed-072b5b3af109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Divide dataset into training and testing\n",
        "train_data, test_data = train_test_split(dataset_df[1].values, test_size=0.2)\n",
        "train_data, validation_data = train_test_split(train_data, test_size=0.2)\n",
        "print(len(train_data))\n",
        "print(len(test_data))\n",
        "print(len(validation_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6480\n",
            "2026\n",
            "1620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtYoUI_zh_Ig",
        "colab_type": "text"
      },
      "source": [
        "# Create Embedding weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCcFWnM-gsba",
        "colab_type": "code",
        "outputId": "459fd060-1d03-4571-dea1-0491d6928f90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# One hot array representation\n",
        "embedding_weights = []\n",
        "embedding_weights.append(np.zeros(vocab_size))\n",
        "for char, i in tk.word_index.items():\n",
        "    onehot = np.zeros(vocab_size)\n",
        "    onehot[i-1] = 1\n",
        "    embedding_weights.append(onehot)\n",
        "\n",
        "embedding_weights = np.array(embedding_weights)\n",
        "print(embedding_weights.shape)\n",
        "print(embedding_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(129, 128)\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a90OG4EUiKdk",
        "colab_type": "text"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhUZ2ZoPiNhw",
        "colab_type": "text"
      },
      "source": [
        "## Defined Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2FGzW7siAbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = INPUT_SIZE       # Must be same as an argument max_len inside pad_sequence method\n",
        "embedding_size = vocab_size   # vocab size 128\n",
        "optimizer = 'adam'\n",
        "loss = 'mse'\n",
        "dropout = 0.2\n",
        "batch_size = 64\n",
        "epochs = 500\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5S5wh4oidqi",
        "colab_type": "text"
      },
      "source": [
        "## Input and Embedding Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRy232nRiP5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input layer (shape=(None, 200))\n",
        "inputs = Input(shape=(input_size,), name='input_layer', dtype='int32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn3GNt2Sim2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embedding layer (input_dim=129, output_dim=128, input_length=200, weights=(129, 128))\n",
        "embedding_layer = Embedding(vocab_size + 1, embedding_size, input_length=input_size, weights=[embedding_weights])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRCCZ9sLiqSp",
        "colab_type": "code",
        "outputId": "14d0ef5f-16bf-4483-f230-58f80de2c3b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "x = embedding_layer(inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sEZuZDJGlT9",
        "colab_type": "code",
        "outputId": "91b46a26-dfa6-4439-c892-a44ecb3bca27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_layer.name"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'embedding_1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT-k6FOxiwSL",
        "colab_type": "text"
      },
      "source": [
        "## Encoder layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSRrjZ5gisWw",
        "colab_type": "code",
        "outputId": "b2a8467e-27df-4477-e23f-27515dab5557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "x = Conv1D(128, 5, activation='relu', padding=\"same\")(x)\n",
        "x = MaxPooling1D(pool_size=5)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
        "x = MaxPooling1D(pool_size=4, padding=\"same\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Conv1D(128, 5, activation='relu', padding='same', name='conv_encoder')(x)\n",
        "x = BatchNormalization()(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZPmNNd2i5YW",
        "colab_type": "text"
      },
      "source": [
        "## Decoder layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQyQYgkei9pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling1D(4)(x)\n",
        "x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = UpSampling1D(5)(x)\n",
        "x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o94t89_1i5IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fully connected layer\n",
        "x = Flatten()(x)\n",
        "x = Dense(200, activation='relu')(x)\n",
        "predictions = x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXpTKMU-jS7s",
        "colab_type": "text"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBcE10oPjRXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build and compile model\n",
        "autoencoder = Model(inputs=inputs, outputs=predictions)\n",
        "# opt = adam, loss = mse\n",
        "autoencoder.compile(loss=loss, optimizer = optimizer,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW0srmyPjSUX",
        "colab_type": "code",
        "outputId": "d9943223-c08b-4a2f-97a2-8770eaf3aea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "# Print summary of the model\n",
        "autoencoder.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_layer (InputLayer)     (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 200, 128)          16512     \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 200, 128)          82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 40, 128)           512       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv_encoder (Conv1D)        (None, 40, 128)           82048     \n",
            "_________________________________________________________________\n",
            "up_sampling1d_1 (UpSampling1 (None, 200, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 200, 128)          82048     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 200, 128)          512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 200, 128)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 25600)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 200)               5120200   \n",
            "=================================================================\n",
            "Total params: 5,383,880\n",
            "Trainable params: 5,383,368\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX9Vh2ojtizS",
        "colab_type": "text"
      },
      "source": [
        "## Setup Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYsNmFRftvg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tensorboard = TensorBoard(log_dir=TENSORBOARD_LOGS_PATH, \n",
        "                          histogram_freq=0,\n",
        "                          write_graph=True,\n",
        "                          write_images=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAzU--5ItiDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', \n",
        "                           min_delta=0.01, \n",
        "                           patience=5, # Num of epochs with no improvement after which training stops\n",
        "                           verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4zuIaCD-9cd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = ModelCheckpoint(MAIN_FOLDER + CHECK_POINT_PATH, monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdLNYBtSjjKy",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqry04DIlseX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert string to index \n",
        "train_sequences = tk.texts_to_sequences(train_data)\n",
        "test_sequences = tk.texts_to_sequences(test_data)\n",
        "validation_sequences = tk.texts_to_sequences(validation_data)\n",
        "\n",
        "# Padding\n",
        "train = pad_sequences(train_sequences, maxlen=200, padding='post')\n",
        "test = pad_sequences(test_sequences, maxlen=200, padding='post')\n",
        "validation = pad_sequences(validation_sequences, maxlen=200, padding='post')\n",
        "\n",
        "# Convert to numpy array\n",
        "train = np.array(train, dtype='float32')\n",
        "test = np.array(test, dtype='float32')\n",
        "validation = np.array(validation, dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTNyUpwIklnN",
        "colab_type": "code",
        "outputId": "06b85125-3b7d-4cc6-8aa9-319e54c3e306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(len(train))\n",
        "print(len(test_data))\n",
        "print(len(validation_data))\n",
        "\n",
        "x_val = validation\n",
        "y_val = validation\n",
        "\n",
        "x_train = train\n",
        "y_train = train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6480\n",
            "2026\n",
            "1620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBB6uq2AjSYE",
        "colab_type": "code",
        "outputId": "759123ae-cbe4-462c-fdd0-f0e1cff66ece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34840
        }
      },
      "source": [
        "autoencoder_train = autoencoder.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 6480 samples, validate on 1620 samples\n",
            "Epoch 1/500\n",
            "6480/6480 [==============================] - 5s 725us/step - loss: 1282.4423 - acc: 0.0025 - val_loss: 665.6196 - val_acc: 0.0037\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.00370, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 2/500\n",
            "6480/6480 [==============================] - 2s 260us/step - loss: 510.5454 - acc: 0.0019 - val_loss: 446.5192 - val_acc: 0.0012\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.00370\n",
            "Epoch 3/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 392.8922 - acc: 0.0042 - val_loss: 363.4610 - val_acc: 0.0025\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.00370\n",
            "Epoch 4/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 340.0446 - acc: 0.0042 - val_loss: 322.0000 - val_acc: 0.0043\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.00370 to 0.00432, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 5/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 274.5104 - acc: 0.0069 - val_loss: 229.7839 - val_acc: 0.0160\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.00432 to 0.01605, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 6/500\n",
            "6480/6480 [==============================] - 2s 262us/step - loss: 194.6122 - acc: 0.0096 - val_loss: 148.1977 - val_acc: 0.0074\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.01605\n",
            "Epoch 7/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 127.1238 - acc: 0.0174 - val_loss: 115.9468 - val_acc: 0.0247\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.01605 to 0.02469, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 8/500\n",
            "6480/6480 [==============================] - 2s 269us/step - loss: 92.3596 - acc: 0.0461 - val_loss: 72.2369 - val_acc: 0.0488\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.02469 to 0.04877, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 9/500\n",
            "6480/6480 [==============================] - 2s 264us/step - loss: 79.7957 - acc: 0.0727 - val_loss: 56.4563 - val_acc: 0.0648\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.04877 to 0.06481, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 10/500\n",
            "6480/6480 [==============================] - 2s 262us/step - loss: 68.0839 - acc: 0.0944 - val_loss: 45.6847 - val_acc: 0.0753\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.06481 to 0.07531, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 11/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 61.3799 - acc: 0.1103 - val_loss: 43.0225 - val_acc: 0.1000\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.07531 to 0.10000, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 12/500\n",
            "6480/6480 [==============================] - 2s 264us/step - loss: 56.6402 - acc: 0.1181 - val_loss: 41.8809 - val_acc: 0.1673\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.10000 to 0.16728, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 13/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 51.7108 - acc: 0.1301 - val_loss: 32.2543 - val_acc: 0.1586\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.16728\n",
            "Epoch 14/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 48.5474 - acc: 0.1336 - val_loss: 42.4058 - val_acc: 0.1562\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.16728\n",
            "Epoch 15/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 47.3734 - acc: 0.1420 - val_loss: 26.8156 - val_acc: 0.1864\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.16728 to 0.18642, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 16/500\n",
            "6480/6480 [==============================] - 2s 259us/step - loss: 43.1638 - acc: 0.1485 - val_loss: 26.2174 - val_acc: 0.1753\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.18642\n",
            "Epoch 17/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 41.8465 - acc: 0.1452 - val_loss: 24.1287 - val_acc: 0.1704\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.18642\n",
            "Epoch 18/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 39.8225 - acc: 0.1590 - val_loss: 22.3228 - val_acc: 0.2438\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.18642 to 0.24383, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 19/500\n",
            "6480/6480 [==============================] - 2s 258us/step - loss: 39.1385 - acc: 0.1647 - val_loss: 24.0579 - val_acc: 0.2494\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.24383 to 0.24938, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 20/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 36.1483 - acc: 0.1710 - val_loss: 29.6628 - val_acc: 0.2383\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.24938\n",
            "Epoch 21/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 34.6282 - acc: 0.1642 - val_loss: 20.6588 - val_acc: 0.2278\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.24938\n",
            "Epoch 22/500\n",
            "6480/6480 [==============================] - 2s 249us/step - loss: 33.5821 - acc: 0.1667 - val_loss: 18.7666 - val_acc: 0.2630\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.24938 to 0.26296, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 23/500\n",
            "6480/6480 [==============================] - 2s 258us/step - loss: 32.6935 - acc: 0.1716 - val_loss: 18.6848 - val_acc: 0.2395\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.26296\n",
            "Epoch 24/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 32.2311 - acc: 0.1756 - val_loss: 22.0408 - val_acc: 0.2735\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.26296 to 0.27346, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 25/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 31.9672 - acc: 0.1802 - val_loss: 19.9308 - val_acc: 0.2265\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.27346\n",
            "Epoch 26/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 30.4921 - acc: 0.1775 - val_loss: 17.1200 - val_acc: 0.2963\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.27346 to 0.29630, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 27/500\n",
            "6480/6480 [==============================] - 2s 263us/step - loss: 28.7756 - acc: 0.1861 - val_loss: 16.7945 - val_acc: 0.2123\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.29630\n",
            "Epoch 28/500\n",
            "6480/6480 [==============================] - 2s 259us/step - loss: 29.4243 - acc: 0.1850 - val_loss: 16.5671 - val_acc: 0.2827\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.29630\n",
            "Epoch 29/500\n",
            "6480/6480 [==============================] - 2s 258us/step - loss: 27.8848 - acc: 0.1864 - val_loss: 20.8507 - val_acc: 0.1932\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.29630\n",
            "Epoch 30/500\n",
            "6480/6480 [==============================] - 2s 261us/step - loss: 27.8488 - acc: 0.1875 - val_loss: 16.5799 - val_acc: 0.2938\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.29630\n",
            "Epoch 31/500\n",
            "6480/6480 [==============================] - 2s 259us/step - loss: 27.1598 - acc: 0.1923 - val_loss: 16.6489 - val_acc: 0.2617\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.29630\n",
            "Epoch 32/500\n",
            "6480/6480 [==============================] - 2s 258us/step - loss: 26.7135 - acc: 0.1892 - val_loss: 17.3041 - val_acc: 0.2401\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.29630\n",
            "Epoch 33/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 25.8560 - acc: 0.1897 - val_loss: 14.9586 - val_acc: 0.2840\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.29630\n",
            "Epoch 34/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 25.8464 - acc: 0.1941 - val_loss: 15.6322 - val_acc: 0.2827\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.29630\n",
            "Epoch 35/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 25.5572 - acc: 0.1981 - val_loss: 14.3955 - val_acc: 0.2568\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.29630\n",
            "Epoch 36/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 24.8663 - acc: 0.1998 - val_loss: 14.7820 - val_acc: 0.3179\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.29630 to 0.31790, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 37/500\n",
            "6480/6480 [==============================] - 2s 270us/step - loss: 24.1665 - acc: 0.2014 - val_loss: 14.4328 - val_acc: 0.3136\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.31790\n",
            "Epoch 38/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 23.9479 - acc: 0.2100 - val_loss: 13.2475 - val_acc: 0.3278\n",
            "\n",
            "Epoch 00038: val_acc improved from 0.31790 to 0.32778, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 39/500\n",
            "6480/6480 [==============================] - 2s 264us/step - loss: 23.6514 - acc: 0.1994 - val_loss: 15.0142 - val_acc: 0.3222\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.32778\n",
            "Epoch 40/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 23.7075 - acc: 0.2173 - val_loss: 16.5911 - val_acc: 0.3228\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.32778\n",
            "Epoch 41/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 23.5068 - acc: 0.2091 - val_loss: 14.0778 - val_acc: 0.2407\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.32778\n",
            "Epoch 42/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 22.8350 - acc: 0.2151 - val_loss: 16.4536 - val_acc: 0.2574\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.32778\n",
            "Epoch 43/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 22.4841 - acc: 0.2239 - val_loss: 13.5944 - val_acc: 0.2809\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.32778\n",
            "Epoch 44/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 22.8326 - acc: 0.2131 - val_loss: 13.9412 - val_acc: 0.3111\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.32778\n",
            "Epoch 45/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 22.1758 - acc: 0.2133 - val_loss: 14.1377 - val_acc: 0.3111\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.32778\n",
            "Epoch 46/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 21.5788 - acc: 0.2153 - val_loss: 15.7739 - val_acc: 0.3253\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.32778\n",
            "Epoch 47/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 21.8582 - acc: 0.2265 - val_loss: 15.2991 - val_acc: 0.3222\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.32778\n",
            "Epoch 48/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 20.8199 - acc: 0.2162 - val_loss: 13.1229 - val_acc: 0.2778\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.32778\n",
            "Epoch 49/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 21.0641 - acc: 0.2279 - val_loss: 14.7231 - val_acc: 0.3778\n",
            "\n",
            "Epoch 00049: val_acc improved from 0.32778 to 0.37778, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 50/500\n",
            "6480/6480 [==============================] - 2s 258us/step - loss: 20.8184 - acc: 0.2250 - val_loss: 12.5662 - val_acc: 0.2840\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.37778\n",
            "Epoch 51/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 20.1966 - acc: 0.2278 - val_loss: 12.5468 - val_acc: 0.3346\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.37778\n",
            "Epoch 52/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 20.5438 - acc: 0.2264 - val_loss: 11.5866 - val_acc: 0.3241\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.37778\n",
            "Epoch 53/500\n",
            "6480/6480 [==============================] - 2s 250us/step - loss: 20.2522 - acc: 0.2248 - val_loss: 11.7482 - val_acc: 0.3864\n",
            "\n",
            "Epoch 00053: val_acc improved from 0.37778 to 0.38642, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 54/500\n",
            "6480/6480 [==============================] - 2s 260us/step - loss: 19.8141 - acc: 0.2307 - val_loss: 11.2860 - val_acc: 0.3611\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.38642\n",
            "Epoch 55/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 19.8185 - acc: 0.2148 - val_loss: 15.1123 - val_acc: 0.3617\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.38642\n",
            "Epoch 56/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 19.6846 - acc: 0.2316 - val_loss: 12.3313 - val_acc: 0.3228\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.38642\n",
            "Epoch 57/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 19.3840 - acc: 0.2338 - val_loss: 11.3979 - val_acc: 0.3438\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.38642\n",
            "Epoch 58/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 19.2984 - acc: 0.2301 - val_loss: 11.3027 - val_acc: 0.3228\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.38642\n",
            "Epoch 59/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 18.8655 - acc: 0.2302 - val_loss: 11.6679 - val_acc: 0.3574\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.38642\n",
            "Epoch 60/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 18.4562 - acc: 0.2389 - val_loss: 13.7286 - val_acc: 0.3037\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.38642\n",
            "Epoch 61/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 18.8836 - acc: 0.2346 - val_loss: 11.9247 - val_acc: 0.3167\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.38642\n",
            "Epoch 62/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 19.1378 - acc: 0.2332 - val_loss: 11.0683 - val_acc: 0.3105\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.38642\n",
            "Epoch 63/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 18.2151 - acc: 0.2434 - val_loss: 10.4776 - val_acc: 0.3543\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.38642\n",
            "Epoch 64/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 19.0509 - acc: 0.2400 - val_loss: 11.0875 - val_acc: 0.3315\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.38642\n",
            "Epoch 65/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 18.3921 - acc: 0.2449 - val_loss: 10.8991 - val_acc: 0.3179\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.38642\n",
            "Epoch 66/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 18.1693 - acc: 0.2387 - val_loss: 10.7700 - val_acc: 0.3679\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.38642\n",
            "Epoch 67/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 17.7785 - acc: 0.2417 - val_loss: 11.9952 - val_acc: 0.3364\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.38642\n",
            "Epoch 68/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 17.4709 - acc: 0.2511 - val_loss: 10.1540 - val_acc: 0.3642\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.38642\n",
            "Epoch 69/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 17.7520 - acc: 0.2435 - val_loss: 10.3835 - val_acc: 0.3506\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.38642\n",
            "Epoch 70/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 17.2372 - acc: 0.2440 - val_loss: 12.2647 - val_acc: 0.3247\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.38642\n",
            "Epoch 71/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 17.3305 - acc: 0.2373 - val_loss: 11.6750 - val_acc: 0.3074\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.38642\n",
            "Epoch 72/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 17.3692 - acc: 0.2392 - val_loss: 17.2047 - val_acc: 0.3593\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.38642\n",
            "Epoch 73/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 17.6183 - acc: 0.2444 - val_loss: 9.6263 - val_acc: 0.3123\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.38642\n",
            "Epoch 74/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 17.2011 - acc: 0.2409 - val_loss: 10.2193 - val_acc: 0.3654\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.38642\n",
            "Epoch 75/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 16.9059 - acc: 0.2497 - val_loss: 9.4704 - val_acc: 0.3272\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.38642\n",
            "Epoch 76/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 16.7813 - acc: 0.2434 - val_loss: 11.1077 - val_acc: 0.3309\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.38642\n",
            "Epoch 77/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 16.3851 - acc: 0.2466 - val_loss: 9.8361 - val_acc: 0.3827\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.38642\n",
            "Epoch 78/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 16.2168 - acc: 0.2454 - val_loss: 9.6647 - val_acc: 0.3309\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.38642\n",
            "Epoch 79/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 16.4828 - acc: 0.2454 - val_loss: 9.3351 - val_acc: 0.3883\n",
            "\n",
            "Epoch 00079: val_acc improved from 0.38642 to 0.38827, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 80/500\n",
            "6480/6480 [==============================] - 2s 267us/step - loss: 16.3800 - acc: 0.2576 - val_loss: 10.2699 - val_acc: 0.3617\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.38827\n",
            "Epoch 81/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 16.0222 - acc: 0.2454 - val_loss: 9.9312 - val_acc: 0.3062\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.38827\n",
            "Epoch 82/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 16.7589 - acc: 0.2429 - val_loss: 10.3821 - val_acc: 0.3549\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.38827\n",
            "Epoch 83/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 16.2141 - acc: 0.2491 - val_loss: 9.0410 - val_acc: 0.3759\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.38827\n",
            "Epoch 84/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 15.9684 - acc: 0.2512 - val_loss: 9.6515 - val_acc: 0.3377\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.38827\n",
            "Epoch 85/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 16.0477 - acc: 0.2414 - val_loss: 9.5333 - val_acc: 0.4074\n",
            "\n",
            "Epoch 00085: val_acc improved from 0.38827 to 0.40741, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 86/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 15.7775 - acc: 0.2590 - val_loss: 9.0835 - val_acc: 0.3179\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.40741\n",
            "Epoch 87/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 15.6785 - acc: 0.2517 - val_loss: 9.3566 - val_acc: 0.3691\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.40741\n",
            "Epoch 88/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 15.4554 - acc: 0.2647 - val_loss: 9.5767 - val_acc: 0.2969\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.40741\n",
            "Epoch 89/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 15.3368 - acc: 0.2600 - val_loss: 8.7570 - val_acc: 0.4228\n",
            "\n",
            "Epoch 00089: val_acc improved from 0.40741 to 0.42284, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 90/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 15.2714 - acc: 0.2468 - val_loss: 9.6114 - val_acc: 0.4080\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.42284\n",
            "Epoch 91/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 15.0684 - acc: 0.2511 - val_loss: 9.9171 - val_acc: 0.3846\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.42284\n",
            "Epoch 92/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 15.1843 - acc: 0.2645 - val_loss: 8.2936 - val_acc: 0.3642\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.42284\n",
            "Epoch 93/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 15.6207 - acc: 0.2619 - val_loss: 9.5196 - val_acc: 0.3599\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.42284\n",
            "Epoch 94/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 15.3121 - acc: 0.2563 - val_loss: 10.3613 - val_acc: 0.3951\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.42284\n",
            "Epoch 95/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 15.2239 - acc: 0.2647 - val_loss: 9.5831 - val_acc: 0.3914\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.42284\n",
            "Epoch 96/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 15.0426 - acc: 0.2583 - val_loss: 8.0823 - val_acc: 0.4198\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.42284\n",
            "Epoch 97/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 14.7894 - acc: 0.2651 - val_loss: 8.9342 - val_acc: 0.3593\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.42284\n",
            "Epoch 98/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 14.9761 - acc: 0.2619 - val_loss: 8.5232 - val_acc: 0.3802\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.42284\n",
            "Epoch 99/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 14.9123 - acc: 0.2586 - val_loss: 8.7513 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.42284\n",
            "Epoch 100/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 14.4910 - acc: 0.2665 - val_loss: 8.8826 - val_acc: 0.3809\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.42284\n",
            "Epoch 101/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 14.6134 - acc: 0.2693 - val_loss: 8.3628 - val_acc: 0.3889\n",
            "\n",
            "Epoch 00101: val_acc did not improve from 0.42284\n",
            "Epoch 102/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 14.4590 - acc: 0.2651 - val_loss: 8.6719 - val_acc: 0.3716\n",
            "\n",
            "Epoch 00102: val_acc did not improve from 0.42284\n",
            "Epoch 103/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 14.4263 - acc: 0.2574 - val_loss: 8.4773 - val_acc: 0.4309\n",
            "\n",
            "Epoch 00103: val_acc improved from 0.42284 to 0.43086, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 104/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 14.3452 - acc: 0.2659 - val_loss: 8.9801 - val_acc: 0.4272\n",
            "\n",
            "Epoch 00104: val_acc did not improve from 0.43086\n",
            "Epoch 105/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 14.3525 - acc: 0.2668 - val_loss: 8.0214 - val_acc: 0.4142\n",
            "\n",
            "Epoch 00105: val_acc did not improve from 0.43086\n",
            "Epoch 106/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 14.3529 - acc: 0.2782 - val_loss: 8.1259 - val_acc: 0.3321\n",
            "\n",
            "Epoch 00106: val_acc did not improve from 0.43086\n",
            "Epoch 107/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 14.2585 - acc: 0.2755 - val_loss: 8.5925 - val_acc: 0.3173\n",
            "\n",
            "Epoch 00107: val_acc did not improve from 0.43086\n",
            "Epoch 108/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 14.1888 - acc: 0.2693 - val_loss: 8.1602 - val_acc: 0.3660\n",
            "\n",
            "Epoch 00108: val_acc did not improve from 0.43086\n",
            "Epoch 109/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 14.0586 - acc: 0.2673 - val_loss: 8.4582 - val_acc: 0.3377\n",
            "\n",
            "Epoch 00109: val_acc did not improve from 0.43086\n",
            "Epoch 110/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 14.0265 - acc: 0.2733 - val_loss: 8.3197 - val_acc: 0.3963\n",
            "\n",
            "Epoch 00110: val_acc did not improve from 0.43086\n",
            "Epoch 111/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 13.7144 - acc: 0.2656 - val_loss: 8.6843 - val_acc: 0.3981\n",
            "\n",
            "Epoch 00111: val_acc did not improve from 0.43086\n",
            "Epoch 112/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 13.9248 - acc: 0.2731 - val_loss: 7.9854 - val_acc: 0.3969\n",
            "\n",
            "Epoch 00112: val_acc did not improve from 0.43086\n",
            "Epoch 113/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 13.7836 - acc: 0.2722 - val_loss: 8.1762 - val_acc: 0.3858\n",
            "\n",
            "Epoch 00113: val_acc did not improve from 0.43086\n",
            "Epoch 114/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 13.7029 - acc: 0.2673 - val_loss: 8.1216 - val_acc: 0.3704\n",
            "\n",
            "Epoch 00114: val_acc did not improve from 0.43086\n",
            "Epoch 115/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 13.7635 - acc: 0.2753 - val_loss: 7.7807 - val_acc: 0.3574\n",
            "\n",
            "Epoch 00115: val_acc did not improve from 0.43086\n",
            "Epoch 116/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 13.8360 - acc: 0.2756 - val_loss: 8.5964 - val_acc: 0.3821\n",
            "\n",
            "Epoch 00116: val_acc did not improve from 0.43086\n",
            "Epoch 117/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 13.6062 - acc: 0.2724 - val_loss: 8.9352 - val_acc: 0.2840\n",
            "\n",
            "Epoch 00117: val_acc did not improve from 0.43086\n",
            "Epoch 118/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 13.7076 - acc: 0.2696 - val_loss: 9.6940 - val_acc: 0.3605\n",
            "\n",
            "Epoch 00118: val_acc did not improve from 0.43086\n",
            "Epoch 119/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 13.5645 - acc: 0.2741 - val_loss: 9.0265 - val_acc: 0.3815\n",
            "\n",
            "Epoch 00119: val_acc did not improve from 0.43086\n",
            "Epoch 120/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 13.6978 - acc: 0.2693 - val_loss: 7.5374 - val_acc: 0.4012\n",
            "\n",
            "Epoch 00120: val_acc did not improve from 0.43086\n",
            "Epoch 121/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 13.6586 - acc: 0.2705 - val_loss: 8.7435 - val_acc: 0.4142\n",
            "\n",
            "Epoch 00121: val_acc did not improve from 0.43086\n",
            "Epoch 122/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 13.4043 - acc: 0.2653 - val_loss: 7.1920 - val_acc: 0.3994\n",
            "\n",
            "Epoch 00122: val_acc did not improve from 0.43086\n",
            "Epoch 123/500\n",
            "6480/6480 [==============================] - 2s 250us/step - loss: 13.2431 - acc: 0.2776 - val_loss: 8.5650 - val_acc: 0.3290\n",
            "\n",
            "Epoch 00123: val_acc did not improve from 0.43086\n",
            "Epoch 124/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 13.6281 - acc: 0.2753 - val_loss: 7.8078 - val_acc: 0.4025\n",
            "\n",
            "Epoch 00124: val_acc did not improve from 0.43086\n",
            "Epoch 125/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 13.6346 - acc: 0.2676 - val_loss: 7.7762 - val_acc: 0.3426\n",
            "\n",
            "Epoch 00125: val_acc did not improve from 0.43086\n",
            "Epoch 126/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 13.4372 - acc: 0.2804 - val_loss: 8.2520 - val_acc: 0.3895\n",
            "\n",
            "Epoch 00126: val_acc did not improve from 0.43086\n",
            "Epoch 127/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 13.2720 - acc: 0.2736 - val_loss: 7.4521 - val_acc: 0.3981\n",
            "\n",
            "Epoch 00127: val_acc did not improve from 0.43086\n",
            "Epoch 128/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 13.0811 - acc: 0.2827 - val_loss: 11.5791 - val_acc: 0.2611\n",
            "\n",
            "Epoch 00128: val_acc did not improve from 0.43086\n",
            "Epoch 129/500\n",
            "6480/6480 [==============================] - 2s 249us/step - loss: 13.8924 - acc: 0.2713 - val_loss: 9.9530 - val_acc: 0.3895\n",
            "\n",
            "Epoch 00129: val_acc did not improve from 0.43086\n",
            "Epoch 130/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 12.9529 - acc: 0.2762 - val_loss: 6.8957 - val_acc: 0.4272\n",
            "\n",
            "Epoch 00130: val_acc did not improve from 0.43086\n",
            "Epoch 131/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 13.1062 - acc: 0.2920 - val_loss: 7.3340 - val_acc: 0.4370\n",
            "\n",
            "Epoch 00131: val_acc improved from 0.43086 to 0.43704, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 132/500\n",
            "6480/6480 [==============================] - 2s 258us/step - loss: 13.0630 - acc: 0.2779 - val_loss: 7.8270 - val_acc: 0.4160\n",
            "\n",
            "Epoch 00132: val_acc did not improve from 0.43704\n",
            "Epoch 133/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 13.2543 - acc: 0.2789 - val_loss: 7.0815 - val_acc: 0.3969\n",
            "\n",
            "Epoch 00133: val_acc did not improve from 0.43704\n",
            "Epoch 134/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 12.9275 - acc: 0.2807 - val_loss: 6.9360 - val_acc: 0.3642\n",
            "\n",
            "Epoch 00134: val_acc did not improve from 0.43704\n",
            "Epoch 135/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 12.7392 - acc: 0.2741 - val_loss: 6.9612 - val_acc: 0.4062\n",
            "\n",
            "Epoch 00135: val_acc did not improve from 0.43704\n",
            "Epoch 136/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 12.9752 - acc: 0.2812 - val_loss: 8.5557 - val_acc: 0.4099\n",
            "\n",
            "Epoch 00136: val_acc did not improve from 0.43704\n",
            "Epoch 137/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 12.8977 - acc: 0.2843 - val_loss: 9.5852 - val_acc: 0.3883\n",
            "\n",
            "Epoch 00137: val_acc did not improve from 0.43704\n",
            "Epoch 138/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 12.7398 - acc: 0.2796 - val_loss: 7.4165 - val_acc: 0.4333\n",
            "\n",
            "Epoch 00138: val_acc did not improve from 0.43704\n",
            "Epoch 139/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 12.9065 - acc: 0.2864 - val_loss: 6.9844 - val_acc: 0.4228\n",
            "\n",
            "Epoch 00139: val_acc did not improve from 0.43704\n",
            "Epoch 140/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 12.4481 - acc: 0.2864 - val_loss: 7.3099 - val_acc: 0.3938\n",
            "\n",
            "Epoch 00140: val_acc did not improve from 0.43704\n",
            "Epoch 141/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 12.6542 - acc: 0.2850 - val_loss: 7.4731 - val_acc: 0.4148\n",
            "\n",
            "Epoch 00141: val_acc did not improve from 0.43704\n",
            "Epoch 142/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 12.6850 - acc: 0.2807 - val_loss: 7.8360 - val_acc: 0.3963\n",
            "\n",
            "Epoch 00142: val_acc did not improve from 0.43704\n",
            "Epoch 143/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 12.3710 - acc: 0.2807 - val_loss: 7.0467 - val_acc: 0.3765\n",
            "\n",
            "Epoch 00143: val_acc did not improve from 0.43704\n",
            "Epoch 144/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 12.2917 - acc: 0.2946 - val_loss: 7.4719 - val_acc: 0.3605\n",
            "\n",
            "Epoch 00144: val_acc did not improve from 0.43704\n",
            "Epoch 145/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 12.5511 - acc: 0.2866 - val_loss: 7.3456 - val_acc: 0.3833\n",
            "\n",
            "Epoch 00145: val_acc did not improve from 0.43704\n",
            "Epoch 146/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 12.4837 - acc: 0.2938 - val_loss: 8.2247 - val_acc: 0.3642\n",
            "\n",
            "Epoch 00146: val_acc did not improve from 0.43704\n",
            "Epoch 147/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 12.3159 - acc: 0.2986 - val_loss: 6.8185 - val_acc: 0.3543\n",
            "\n",
            "Epoch 00147: val_acc did not improve from 0.43704\n",
            "Epoch 148/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 12.7009 - acc: 0.2826 - val_loss: 7.1220 - val_acc: 0.3957\n",
            "\n",
            "Epoch 00148: val_acc did not improve from 0.43704\n",
            "Epoch 149/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 12.4972 - acc: 0.2850 - val_loss: 8.1855 - val_acc: 0.3852\n",
            "\n",
            "Epoch 00149: val_acc did not improve from 0.43704\n",
            "Epoch 150/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 12.6368 - acc: 0.2809 - val_loss: 7.2422 - val_acc: 0.4364\n",
            "\n",
            "Epoch 00150: val_acc did not improve from 0.43704\n",
            "Epoch 151/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 12.4163 - acc: 0.2943 - val_loss: 6.5854 - val_acc: 0.3864\n",
            "\n",
            "Epoch 00151: val_acc did not improve from 0.43704\n",
            "Epoch 152/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 12.6874 - acc: 0.2924 - val_loss: 9.8318 - val_acc: 0.4043\n",
            "\n",
            "Epoch 00152: val_acc did not improve from 0.43704\n",
            "Epoch 153/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 12.6121 - acc: 0.2866 - val_loss: 7.0372 - val_acc: 0.4389\n",
            "\n",
            "Epoch 00153: val_acc improved from 0.43704 to 0.43889, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 154/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 12.1268 - acc: 0.2969 - val_loss: 8.9639 - val_acc: 0.3586\n",
            "\n",
            "Epoch 00154: val_acc did not improve from 0.43889\n",
            "Epoch 155/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 12.4605 - acc: 0.2858 - val_loss: 6.5414 - val_acc: 0.4272\n",
            "\n",
            "Epoch 00155: val_acc did not improve from 0.43889\n",
            "Epoch 156/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 12.2073 - acc: 0.2931 - val_loss: 7.6861 - val_acc: 0.3611\n",
            "\n",
            "Epoch 00156: val_acc did not improve from 0.43889\n",
            "Epoch 157/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 12.0512 - acc: 0.2861 - val_loss: 6.8633 - val_acc: 0.4025\n",
            "\n",
            "Epoch 00157: val_acc did not improve from 0.43889\n",
            "Epoch 158/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 11.9310 - acc: 0.2906 - val_loss: 6.7973 - val_acc: 0.3981\n",
            "\n",
            "Epoch 00158: val_acc did not improve from 0.43889\n",
            "Epoch 159/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 12.1821 - acc: 0.2804 - val_loss: 7.3057 - val_acc: 0.4228\n",
            "\n",
            "Epoch 00159: val_acc did not improve from 0.43889\n",
            "Epoch 160/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 11.9980 - acc: 0.2889 - val_loss: 6.9370 - val_acc: 0.3679\n",
            "\n",
            "Epoch 00160: val_acc did not improve from 0.43889\n",
            "Epoch 161/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 12.2728 - acc: 0.2861 - val_loss: 6.8962 - val_acc: 0.3741\n",
            "\n",
            "Epoch 00161: val_acc did not improve from 0.43889\n",
            "Epoch 162/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 11.8788 - acc: 0.2941 - val_loss: 6.4518 - val_acc: 0.3951\n",
            "\n",
            "Epoch 00162: val_acc did not improve from 0.43889\n",
            "Epoch 163/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 11.8714 - acc: 0.2909 - val_loss: 7.1567 - val_acc: 0.4265\n",
            "\n",
            "Epoch 00163: val_acc did not improve from 0.43889\n",
            "Epoch 164/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.7697 - acc: 0.2852 - val_loss: 6.6791 - val_acc: 0.4457\n",
            "\n",
            "Epoch 00164: val_acc improved from 0.43889 to 0.44568, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 165/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 12.0798 - acc: 0.2915 - val_loss: 6.8897 - val_acc: 0.4451\n",
            "\n",
            "Epoch 00165: val_acc did not improve from 0.44568\n",
            "Epoch 166/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.8388 - acc: 0.2944 - val_loss: 6.7109 - val_acc: 0.4105\n",
            "\n",
            "Epoch 00166: val_acc did not improve from 0.44568\n",
            "Epoch 167/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 11.8312 - acc: 0.2998 - val_loss: 6.2455 - val_acc: 0.3889\n",
            "\n",
            "Epoch 00167: val_acc did not improve from 0.44568\n",
            "Epoch 168/500\n",
            "6480/6480 [==============================] - 2s 250us/step - loss: 11.8117 - acc: 0.2927 - val_loss: 6.6445 - val_acc: 0.4407\n",
            "\n",
            "Epoch 00168: val_acc did not improve from 0.44568\n",
            "Epoch 169/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 11.6753 - acc: 0.2966 - val_loss: 7.1323 - val_acc: 0.3543\n",
            "\n",
            "Epoch 00169: val_acc did not improve from 0.44568\n",
            "Epoch 170/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 11.5348 - acc: 0.2932 - val_loss: 6.1091 - val_acc: 0.4512\n",
            "\n",
            "Epoch 00170: val_acc improved from 0.44568 to 0.45123, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 171/500\n",
            "6480/6480 [==============================] - 2s 268us/step - loss: 11.5703 - acc: 0.2926 - val_loss: 6.9590 - val_acc: 0.3753\n",
            "\n",
            "Epoch 00171: val_acc did not improve from 0.45123\n",
            "Epoch 172/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 11.7239 - acc: 0.2926 - val_loss: 6.2322 - val_acc: 0.4543\n",
            "\n",
            "Epoch 00172: val_acc improved from 0.45123 to 0.45432, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 173/500\n",
            "6480/6480 [==============================] - 2s 259us/step - loss: 11.9950 - acc: 0.2915 - val_loss: 7.4925 - val_acc: 0.4210\n",
            "\n",
            "Epoch 00173: val_acc did not improve from 0.45432\n",
            "Epoch 174/500\n",
            "6480/6480 [==============================] - 2s 249us/step - loss: 11.6383 - acc: 0.2940 - val_loss: 6.2566 - val_acc: 0.3821\n",
            "\n",
            "Epoch 00174: val_acc did not improve from 0.45432\n",
            "Epoch 175/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 11.6042 - acc: 0.2926 - val_loss: 7.1175 - val_acc: 0.3877\n",
            "\n",
            "Epoch 00175: val_acc did not improve from 0.45432\n",
            "Epoch 176/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.5277 - acc: 0.2975 - val_loss: 6.2974 - val_acc: 0.4414\n",
            "\n",
            "Epoch 00176: val_acc did not improve from 0.45432\n",
            "Epoch 177/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 11.5358 - acc: 0.2951 - val_loss: 7.7352 - val_acc: 0.3691\n",
            "\n",
            "Epoch 00177: val_acc did not improve from 0.45432\n",
            "Epoch 178/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.7611 - acc: 0.2878 - val_loss: 6.2889 - val_acc: 0.3938\n",
            "\n",
            "Epoch 00178: val_acc did not improve from 0.45432\n",
            "Epoch 179/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.8654 - acc: 0.2977 - val_loss: 6.3786 - val_acc: 0.4327\n",
            "\n",
            "Epoch 00179: val_acc did not improve from 0.45432\n",
            "Epoch 180/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 11.4008 - acc: 0.2958 - val_loss: 6.4627 - val_acc: 0.4130\n",
            "\n",
            "Epoch 00180: val_acc did not improve from 0.45432\n",
            "Epoch 181/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.5502 - acc: 0.2946 - val_loss: 8.0075 - val_acc: 0.4093\n",
            "\n",
            "Epoch 00181: val_acc did not improve from 0.45432\n",
            "Epoch 182/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 11.5029 - acc: 0.2929 - val_loss: 6.1510 - val_acc: 0.4167\n",
            "\n",
            "Epoch 00182: val_acc did not improve from 0.45432\n",
            "Epoch 183/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.6350 - acc: 0.2989 - val_loss: 6.7404 - val_acc: 0.4395\n",
            "\n",
            "Epoch 00183: val_acc did not improve from 0.45432\n",
            "Epoch 184/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 11.4630 - acc: 0.2935 - val_loss: 7.2003 - val_acc: 0.4117\n",
            "\n",
            "Epoch 00184: val_acc did not improve from 0.45432\n",
            "Epoch 185/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.5156 - acc: 0.2992 - val_loss: 6.1023 - val_acc: 0.4179\n",
            "\n",
            "Epoch 00185: val_acc did not improve from 0.45432\n",
            "Epoch 186/500\n",
            "6480/6480 [==============================] - 2s 242us/step - loss: 11.4809 - acc: 0.3015 - val_loss: 7.9916 - val_acc: 0.4031\n",
            "\n",
            "Epoch 00186: val_acc did not improve from 0.45432\n",
            "Epoch 187/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.5508 - acc: 0.3023 - val_loss: 6.7044 - val_acc: 0.3938\n",
            "\n",
            "Epoch 00187: val_acc did not improve from 0.45432\n",
            "Epoch 188/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.3456 - acc: 0.2968 - val_loss: 6.6605 - val_acc: 0.4401\n",
            "\n",
            "Epoch 00188: val_acc did not improve from 0.45432\n",
            "Epoch 189/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 11.5691 - acc: 0.2966 - val_loss: 6.1186 - val_acc: 0.4426\n",
            "\n",
            "Epoch 00189: val_acc did not improve from 0.45432\n",
            "Epoch 190/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 11.3183 - acc: 0.2980 - val_loss: 6.8668 - val_acc: 0.4272\n",
            "\n",
            "Epoch 00190: val_acc did not improve from 0.45432\n",
            "Epoch 191/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 11.4140 - acc: 0.2983 - val_loss: 6.3843 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00191: val_acc did not improve from 0.45432\n",
            "Epoch 192/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.2690 - acc: 0.3062 - val_loss: 9.5029 - val_acc: 0.3691\n",
            "\n",
            "Epoch 00192: val_acc did not improve from 0.45432\n",
            "Epoch 193/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 11.3411 - acc: 0.2974 - val_loss: 5.9582 - val_acc: 0.4383\n",
            "\n",
            "Epoch 00193: val_acc did not improve from 0.45432\n",
            "Epoch 194/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.2892 - acc: 0.3019 - val_loss: 6.3862 - val_acc: 0.4204\n",
            "\n",
            "Epoch 00194: val_acc did not improve from 0.45432\n",
            "Epoch 195/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 11.6412 - acc: 0.2986 - val_loss: 6.9418 - val_acc: 0.3691\n",
            "\n",
            "Epoch 00195: val_acc did not improve from 0.45432\n",
            "Epoch 196/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.2676 - acc: 0.2978 - val_loss: 6.6557 - val_acc: 0.4599\n",
            "\n",
            "Epoch 00196: val_acc improved from 0.45432 to 0.45988, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 197/500\n",
            "6480/6480 [==============================] - 2s 269us/step - loss: 11.2341 - acc: 0.3029 - val_loss: 6.1714 - val_acc: 0.3914\n",
            "\n",
            "Epoch 00197: val_acc did not improve from 0.45988\n",
            "Epoch 198/500\n",
            "6480/6480 [==============================] - 2s 260us/step - loss: 11.3277 - acc: 0.3062 - val_loss: 6.0656 - val_acc: 0.4136\n",
            "\n",
            "Epoch 00198: val_acc did not improve from 0.45988\n",
            "Epoch 199/500\n",
            "6480/6480 [==============================] - 2s 259us/step - loss: 10.9833 - acc: 0.2924 - val_loss: 6.2906 - val_acc: 0.4469\n",
            "\n",
            "Epoch 00199: val_acc did not improve from 0.45988\n",
            "Epoch 200/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 11.2315 - acc: 0.3039 - val_loss: 6.2123 - val_acc: 0.4481\n",
            "\n",
            "Epoch 00200: val_acc did not improve from 0.45988\n",
            "Epoch 201/500\n",
            "6480/6480 [==============================] - 2s 260us/step - loss: 11.0690 - acc: 0.3052 - val_loss: 6.4475 - val_acc: 0.3685\n",
            "\n",
            "Epoch 00201: val_acc did not improve from 0.45988\n",
            "Epoch 202/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 10.9216 - acc: 0.3003 - val_loss: 7.7473 - val_acc: 0.4352\n",
            "\n",
            "Epoch 00202: val_acc did not improve from 0.45988\n",
            "Epoch 203/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 11.0652 - acc: 0.3000 - val_loss: 6.1645 - val_acc: 0.4210\n",
            "\n",
            "Epoch 00203: val_acc did not improve from 0.45988\n",
            "Epoch 204/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 10.9869 - acc: 0.2906 - val_loss: 6.0295 - val_acc: 0.4346\n",
            "\n",
            "Epoch 00204: val_acc did not improve from 0.45988\n",
            "Epoch 205/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 11.0889 - acc: 0.3106 - val_loss: 6.0819 - val_acc: 0.4358\n",
            "\n",
            "Epoch 00205: val_acc did not improve from 0.45988\n",
            "Epoch 206/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 10.9965 - acc: 0.2983 - val_loss: 5.8830 - val_acc: 0.4562\n",
            "\n",
            "Epoch 00206: val_acc did not improve from 0.45988\n",
            "Epoch 207/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 11.1309 - acc: 0.3031 - val_loss: 6.2411 - val_acc: 0.3883\n",
            "\n",
            "Epoch 00207: val_acc did not improve from 0.45988\n",
            "Epoch 208/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 10.8322 - acc: 0.3043 - val_loss: 6.0390 - val_acc: 0.4840\n",
            "\n",
            "Epoch 00208: val_acc improved from 0.45988 to 0.48395, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 209/500\n",
            "6480/6480 [==============================] - 2s 259us/step - loss: 10.8211 - acc: 0.2988 - val_loss: 6.2847 - val_acc: 0.4370\n",
            "\n",
            "Epoch 00209: val_acc did not improve from 0.48395\n",
            "Epoch 210/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.9874 - acc: 0.2965 - val_loss: 5.7018 - val_acc: 0.4432\n",
            "\n",
            "Epoch 00210: val_acc did not improve from 0.48395\n",
            "Epoch 211/500\n",
            "6480/6480 [==============================] - 2s 251us/step - loss: 10.7804 - acc: 0.2986 - val_loss: 6.0428 - val_acc: 0.4352\n",
            "\n",
            "Epoch 00211: val_acc did not improve from 0.48395\n",
            "Epoch 212/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 10.8557 - acc: 0.3002 - val_loss: 6.0471 - val_acc: 0.4204\n",
            "\n",
            "Epoch 00212: val_acc did not improve from 0.48395\n",
            "Epoch 213/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 11.0705 - acc: 0.3076 - val_loss: 6.3856 - val_acc: 0.4037\n",
            "\n",
            "Epoch 00213: val_acc did not improve from 0.48395\n",
            "Epoch 214/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 10.8403 - acc: 0.3154 - val_loss: 6.0869 - val_acc: 0.4543\n",
            "\n",
            "Epoch 00214: val_acc did not improve from 0.48395\n",
            "Epoch 215/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 10.9397 - acc: 0.3060 - val_loss: 6.2487 - val_acc: 0.3883\n",
            "\n",
            "Epoch 00215: val_acc did not improve from 0.48395\n",
            "Epoch 216/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 10.7819 - acc: 0.3102 - val_loss: 6.6441 - val_acc: 0.4191\n",
            "\n",
            "Epoch 00216: val_acc did not improve from 0.48395\n",
            "Epoch 217/500\n",
            "6480/6480 [==============================] - 2s 251us/step - loss: 10.9284 - acc: 0.3069 - val_loss: 6.5994 - val_acc: 0.4395\n",
            "\n",
            "Epoch 00217: val_acc did not improve from 0.48395\n",
            "Epoch 218/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 10.8273 - acc: 0.2991 - val_loss: 6.5729 - val_acc: 0.3895\n",
            "\n",
            "Epoch 00218: val_acc did not improve from 0.48395\n",
            "Epoch 219/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.8209 - acc: 0.3012 - val_loss: 6.3018 - val_acc: 0.3895\n",
            "\n",
            "Epoch 00219: val_acc did not improve from 0.48395\n",
            "Epoch 220/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.6869 - acc: 0.2989 - val_loss: 6.0523 - val_acc: 0.3920\n",
            "\n",
            "Epoch 00220: val_acc did not improve from 0.48395\n",
            "Epoch 221/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 11.0018 - acc: 0.2921 - val_loss: 5.9172 - val_acc: 0.3988\n",
            "\n",
            "Epoch 00221: val_acc did not improve from 0.48395\n",
            "Epoch 222/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 10.6157 - acc: 0.3025 - val_loss: 6.1802 - val_acc: 0.4315\n",
            "\n",
            "Epoch 00222: val_acc did not improve from 0.48395\n",
            "Epoch 223/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 10.7651 - acc: 0.2977 - val_loss: 6.4581 - val_acc: 0.3660\n",
            "\n",
            "Epoch 00223: val_acc did not improve from 0.48395\n",
            "Epoch 224/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 10.7821 - acc: 0.2978 - val_loss: 6.3712 - val_acc: 0.4340\n",
            "\n",
            "Epoch 00224: val_acc did not improve from 0.48395\n",
            "Epoch 225/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 10.5373 - acc: 0.3086 - val_loss: 5.7827 - val_acc: 0.4198\n",
            "\n",
            "Epoch 00225: val_acc did not improve from 0.48395\n",
            "Epoch 226/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.6218 - acc: 0.3079 - val_loss: 6.0292 - val_acc: 0.4506\n",
            "\n",
            "Epoch 00226: val_acc did not improve from 0.48395\n",
            "Epoch 227/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 10.6836 - acc: 0.3074 - val_loss: 6.0763 - val_acc: 0.3975\n",
            "\n",
            "Epoch 00227: val_acc did not improve from 0.48395\n",
            "Epoch 228/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.7810 - acc: 0.3159 - val_loss: 6.9551 - val_acc: 0.3691\n",
            "\n",
            "Epoch 00228: val_acc did not improve from 0.48395\n",
            "Epoch 229/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 10.6276 - acc: 0.3090 - val_loss: 5.7865 - val_acc: 0.4247\n",
            "\n",
            "Epoch 00229: val_acc did not improve from 0.48395\n",
            "Epoch 230/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 10.4779 - acc: 0.3130 - val_loss: 6.3190 - val_acc: 0.4148\n",
            "\n",
            "Epoch 00230: val_acc did not improve from 0.48395\n",
            "Epoch 231/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.7582 - acc: 0.3093 - val_loss: 6.0715 - val_acc: 0.4580\n",
            "\n",
            "Epoch 00231: val_acc did not improve from 0.48395\n",
            "Epoch 232/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.6380 - acc: 0.3139 - val_loss: 6.2420 - val_acc: 0.4654\n",
            "\n",
            "Epoch 00232: val_acc did not improve from 0.48395\n",
            "Epoch 233/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.6804 - acc: 0.2989 - val_loss: 6.1311 - val_acc: 0.4216\n",
            "\n",
            "Epoch 00233: val_acc did not improve from 0.48395\n",
            "Epoch 234/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 10.6300 - acc: 0.3035 - val_loss: 6.9386 - val_acc: 0.4469\n",
            "\n",
            "Epoch 00234: val_acc did not improve from 0.48395\n",
            "Epoch 235/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 10.4751 - acc: 0.3035 - val_loss: 5.4604 - val_acc: 0.4648\n",
            "\n",
            "Epoch 00235: val_acc did not improve from 0.48395\n",
            "Epoch 236/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.4983 - acc: 0.3077 - val_loss: 6.5202 - val_acc: 0.4691\n",
            "\n",
            "Epoch 00236: val_acc did not improve from 0.48395\n",
            "Epoch 237/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.4556 - acc: 0.3093 - val_loss: 5.9203 - val_acc: 0.4531\n",
            "\n",
            "Epoch 00237: val_acc did not improve from 0.48395\n",
            "Epoch 238/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.5253 - acc: 0.3035 - val_loss: 6.0733 - val_acc: 0.4216\n",
            "\n",
            "Epoch 00238: val_acc did not improve from 0.48395\n",
            "Epoch 239/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 10.3747 - acc: 0.3145 - val_loss: 6.8251 - val_acc: 0.4167\n",
            "\n",
            "Epoch 00239: val_acc did not improve from 0.48395\n",
            "Epoch 240/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 10.6686 - acc: 0.3039 - val_loss: 5.5814 - val_acc: 0.4284\n",
            "\n",
            "Epoch 00240: val_acc did not improve from 0.48395\n",
            "Epoch 241/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 10.4355 - acc: 0.3048 - val_loss: 5.5222 - val_acc: 0.4210\n",
            "\n",
            "Epoch 00241: val_acc did not improve from 0.48395\n",
            "Epoch 242/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 10.6541 - acc: 0.3073 - val_loss: 5.8193 - val_acc: 0.4432\n",
            "\n",
            "Epoch 00242: val_acc did not improve from 0.48395\n",
            "Epoch 243/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 10.2619 - acc: 0.3187 - val_loss: 5.6110 - val_acc: 0.4006\n",
            "\n",
            "Epoch 00243: val_acc did not improve from 0.48395\n",
            "Epoch 244/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 10.7613 - acc: 0.2995 - val_loss: 6.1217 - val_acc: 0.4494\n",
            "\n",
            "Epoch 00244: val_acc did not improve from 0.48395\n",
            "Epoch 245/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 10.4883 - acc: 0.3130 - val_loss: 5.9441 - val_acc: 0.4272\n",
            "\n",
            "Epoch 00245: val_acc did not improve from 0.48395\n",
            "Epoch 246/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 10.5417 - acc: 0.3046 - val_loss: 5.4959 - val_acc: 0.4889\n",
            "\n",
            "Epoch 00246: val_acc improved from 0.48395 to 0.48889, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 247/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 10.4251 - acc: 0.3090 - val_loss: 6.0498 - val_acc: 0.4704\n",
            "\n",
            "Epoch 00247: val_acc did not improve from 0.48889\n",
            "Epoch 248/500\n",
            "6480/6480 [==============================] - 2s 249us/step - loss: 10.4461 - acc: 0.3116 - val_loss: 5.9440 - val_acc: 0.4605\n",
            "\n",
            "Epoch 00248: val_acc did not improve from 0.48889\n",
            "Epoch 249/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.3514 - acc: 0.3060 - val_loss: 5.4173 - val_acc: 0.4586\n",
            "\n",
            "Epoch 00249: val_acc did not improve from 0.48889\n",
            "Epoch 250/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.3932 - acc: 0.3026 - val_loss: 5.3089 - val_acc: 0.4617\n",
            "\n",
            "Epoch 00250: val_acc did not improve from 0.48889\n",
            "Epoch 251/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.3802 - acc: 0.3119 - val_loss: 5.7134 - val_acc: 0.4309\n",
            "\n",
            "Epoch 00251: val_acc did not improve from 0.48889\n",
            "Epoch 252/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.4279 - acc: 0.3093 - val_loss: 6.8359 - val_acc: 0.4327\n",
            "\n",
            "Epoch 00252: val_acc did not improve from 0.48889\n",
            "Epoch 253/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 10.4801 - acc: 0.3139 - val_loss: 5.2870 - val_acc: 0.4537\n",
            "\n",
            "Epoch 00253: val_acc did not improve from 0.48889\n",
            "Epoch 254/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.0916 - acc: 0.3000 - val_loss: 5.4069 - val_acc: 0.3920\n",
            "\n",
            "Epoch 00254: val_acc did not improve from 0.48889\n",
            "Epoch 255/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 10.2003 - acc: 0.3148 - val_loss: 5.4267 - val_acc: 0.4457\n",
            "\n",
            "Epoch 00255: val_acc did not improve from 0.48889\n",
            "Epoch 256/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.0903 - acc: 0.3071 - val_loss: 5.8957 - val_acc: 0.4111\n",
            "\n",
            "Epoch 00256: val_acc did not improve from 0.48889\n",
            "Epoch 257/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.2895 - acc: 0.2961 - val_loss: 5.9865 - val_acc: 0.4704\n",
            "\n",
            "Epoch 00257: val_acc did not improve from 0.48889\n",
            "Epoch 258/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 10.5331 - acc: 0.3125 - val_loss: 5.4324 - val_acc: 0.4247\n",
            "\n",
            "Epoch 00258: val_acc did not improve from 0.48889\n",
            "Epoch 259/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 10.1454 - acc: 0.3156 - val_loss: 5.5829 - val_acc: 0.4580\n",
            "\n",
            "Epoch 00259: val_acc did not improve from 0.48889\n",
            "Epoch 260/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 10.0827 - acc: 0.3179 - val_loss: 5.5040 - val_acc: 0.4111\n",
            "\n",
            "Epoch 00260: val_acc did not improve from 0.48889\n",
            "Epoch 261/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 10.3084 - acc: 0.3085 - val_loss: 6.0966 - val_acc: 0.4395\n",
            "\n",
            "Epoch 00261: val_acc did not improve from 0.48889\n",
            "Epoch 262/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 10.2782 - acc: 0.3071 - val_loss: 5.3270 - val_acc: 0.4549\n",
            "\n",
            "Epoch 00262: val_acc did not improve from 0.48889\n",
            "Epoch 263/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 10.2978 - acc: 0.3147 - val_loss: 5.4769 - val_acc: 0.4981\n",
            "\n",
            "Epoch 00263: val_acc improved from 0.48889 to 0.49815, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 264/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 10.2062 - acc: 0.3148 - val_loss: 5.8673 - val_acc: 0.4191\n",
            "\n",
            "Epoch 00264: val_acc did not improve from 0.49815\n",
            "Epoch 265/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 10.4761 - acc: 0.3148 - val_loss: 5.1753 - val_acc: 0.4469\n",
            "\n",
            "Epoch 00265: val_acc did not improve from 0.49815\n",
            "Epoch 266/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 10.0710 - acc: 0.3147 - val_loss: 6.7653 - val_acc: 0.4710\n",
            "\n",
            "Epoch 00266: val_acc did not improve from 0.49815\n",
            "Epoch 267/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 10.0568 - acc: 0.3130 - val_loss: 5.5595 - val_acc: 0.4519\n",
            "\n",
            "Epoch 00267: val_acc did not improve from 0.49815\n",
            "Epoch 268/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.1138 - acc: 0.3157 - val_loss: 6.3004 - val_acc: 0.4599\n",
            "\n",
            "Epoch 00268: val_acc did not improve from 0.49815\n",
            "Epoch 269/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.1319 - acc: 0.3136 - val_loss: 5.5391 - val_acc: 0.4648\n",
            "\n",
            "Epoch 00269: val_acc did not improve from 0.49815\n",
            "Epoch 270/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 10.1094 - acc: 0.3185 - val_loss: 6.1663 - val_acc: 0.3667\n",
            "\n",
            "Epoch 00270: val_acc did not improve from 0.49815\n",
            "Epoch 271/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.0721 - acc: 0.3131 - val_loss: 5.3574 - val_acc: 0.4549\n",
            "\n",
            "Epoch 00271: val_acc did not improve from 0.49815\n",
            "Epoch 272/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 10.1573 - acc: 0.3122 - val_loss: 5.1915 - val_acc: 0.4753\n",
            "\n",
            "Epoch 00272: val_acc did not improve from 0.49815\n",
            "Epoch 273/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 10.1158 - acc: 0.3093 - val_loss: 5.6411 - val_acc: 0.4642\n",
            "\n",
            "Epoch 00273: val_acc did not improve from 0.49815\n",
            "Epoch 274/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.2080 - acc: 0.3116 - val_loss: 5.3333 - val_acc: 0.4556\n",
            "\n",
            "Epoch 00274: val_acc did not improve from 0.49815\n",
            "Epoch 275/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.2075 - acc: 0.3137 - val_loss: 6.6740 - val_acc: 0.4414\n",
            "\n",
            "Epoch 00275: val_acc did not improve from 0.49815\n",
            "Epoch 276/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 10.3604 - acc: 0.3011 - val_loss: 5.7602 - val_acc: 0.4259\n",
            "\n",
            "Epoch 00276: val_acc did not improve from 0.49815\n",
            "Epoch 277/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.0591 - acc: 0.3238 - val_loss: 5.7912 - val_acc: 0.4698\n",
            "\n",
            "Epoch 00277: val_acc did not improve from 0.49815\n",
            "Epoch 278/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 10.0959 - acc: 0.3056 - val_loss: 5.5161 - val_acc: 0.4278\n",
            "\n",
            "Epoch 00278: val_acc did not improve from 0.49815\n",
            "Epoch 279/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 10.1328 - acc: 0.3117 - val_loss: 5.7823 - val_acc: 0.4074\n",
            "\n",
            "Epoch 00279: val_acc did not improve from 0.49815\n",
            "Epoch 280/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.9590 - acc: 0.3205 - val_loss: 5.7016 - val_acc: 0.4086\n",
            "\n",
            "Epoch 00280: val_acc did not improve from 0.49815\n",
            "Epoch 281/500\n",
            "6480/6480 [==============================] - 2s 242us/step - loss: 10.0994 - acc: 0.3156 - val_loss: 5.2636 - val_acc: 0.4377\n",
            "\n",
            "Epoch 00281: val_acc did not improve from 0.49815\n",
            "Epoch 282/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.9128 - acc: 0.3165 - val_loss: 6.0995 - val_acc: 0.4019\n",
            "\n",
            "Epoch 00282: val_acc did not improve from 0.49815\n",
            "Epoch 283/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.9441 - acc: 0.3205 - val_loss: 5.3058 - val_acc: 0.4481\n",
            "\n",
            "Epoch 00283: val_acc did not improve from 0.49815\n",
            "Epoch 284/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 9.9349 - acc: 0.3170 - val_loss: 5.3414 - val_acc: 0.4142\n",
            "\n",
            "Epoch 00284: val_acc did not improve from 0.49815\n",
            "Epoch 285/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.8822 - acc: 0.3171 - val_loss: 5.8395 - val_acc: 0.4142\n",
            "\n",
            "Epoch 00285: val_acc did not improve from 0.49815\n",
            "Epoch 286/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.9215 - acc: 0.3190 - val_loss: 5.1633 - val_acc: 0.4426\n",
            "\n",
            "Epoch 00286: val_acc did not improve from 0.49815\n",
            "Epoch 287/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.9182 - acc: 0.3136 - val_loss: 5.3031 - val_acc: 0.4290\n",
            "\n",
            "Epoch 00287: val_acc did not improve from 0.49815\n",
            "Epoch 288/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.8550 - acc: 0.3119 - val_loss: 6.1918 - val_acc: 0.4358\n",
            "\n",
            "Epoch 00288: val_acc did not improve from 0.49815\n",
            "Epoch 289/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.9616 - acc: 0.3128 - val_loss: 5.8069 - val_acc: 0.4302\n",
            "\n",
            "Epoch 00289: val_acc did not improve from 0.49815\n",
            "Epoch 290/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.9032 - acc: 0.3198 - val_loss: 5.7755 - val_acc: 0.4346\n",
            "\n",
            "Epoch 00290: val_acc did not improve from 0.49815\n",
            "Epoch 291/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.9425 - acc: 0.3230 - val_loss: 5.4906 - val_acc: 0.4191\n",
            "\n",
            "Epoch 00291: val_acc did not improve from 0.49815\n",
            "Epoch 292/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.8608 - acc: 0.3145 - val_loss: 5.8094 - val_acc: 0.4432\n",
            "\n",
            "Epoch 00292: val_acc did not improve from 0.49815\n",
            "Epoch 293/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.8287 - acc: 0.3079 - val_loss: 5.7658 - val_acc: 0.4611\n",
            "\n",
            "Epoch 00293: val_acc did not improve from 0.49815\n",
            "Epoch 294/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 10.0851 - acc: 0.3153 - val_loss: 5.1475 - val_acc: 0.4463\n",
            "\n",
            "Epoch 00294: val_acc did not improve from 0.49815\n",
            "Epoch 295/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.9786 - acc: 0.3191 - val_loss: 5.8858 - val_acc: 0.4543\n",
            "\n",
            "Epoch 00295: val_acc did not improve from 0.49815\n",
            "Epoch 296/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.8479 - acc: 0.3165 - val_loss: 5.0361 - val_acc: 0.4444\n",
            "\n",
            "Epoch 00296: val_acc did not improve from 0.49815\n",
            "Epoch 297/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.7578 - acc: 0.3140 - val_loss: 5.2408 - val_acc: 0.4741\n",
            "\n",
            "Epoch 00297: val_acc did not improve from 0.49815\n",
            "Epoch 298/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.7656 - acc: 0.3230 - val_loss: 5.2877 - val_acc: 0.4636\n",
            "\n",
            "Epoch 00298: val_acc did not improve from 0.49815\n",
            "Epoch 299/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.7991 - acc: 0.3083 - val_loss: 5.0596 - val_acc: 0.4667\n",
            "\n",
            "Epoch 00299: val_acc did not improve from 0.49815\n",
            "Epoch 300/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.6579 - acc: 0.3137 - val_loss: 5.6097 - val_acc: 0.4586\n",
            "\n",
            "Epoch 00300: val_acc did not improve from 0.49815\n",
            "Epoch 301/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.7669 - acc: 0.3231 - val_loss: 5.6148 - val_acc: 0.4636\n",
            "\n",
            "Epoch 00301: val_acc did not improve from 0.49815\n",
            "Epoch 302/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 9.7670 - acc: 0.3221 - val_loss: 5.0272 - val_acc: 0.4290\n",
            "\n",
            "Epoch 00302: val_acc did not improve from 0.49815\n",
            "Epoch 303/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.6130 - acc: 0.3281 - val_loss: 5.1762 - val_acc: 0.4377\n",
            "\n",
            "Epoch 00303: val_acc did not improve from 0.49815\n",
            "Epoch 304/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.8880 - acc: 0.3185 - val_loss: 5.9649 - val_acc: 0.4759\n",
            "\n",
            "Epoch 00304: val_acc did not improve from 0.49815\n",
            "Epoch 305/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.7290 - acc: 0.3154 - val_loss: 5.6616 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00305: val_acc did not improve from 0.49815\n",
            "Epoch 306/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.7034 - acc: 0.3162 - val_loss: 5.1965 - val_acc: 0.4667\n",
            "\n",
            "Epoch 00306: val_acc did not improve from 0.49815\n",
            "Epoch 307/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.7836 - acc: 0.3179 - val_loss: 6.8172 - val_acc: 0.3975\n",
            "\n",
            "Epoch 00307: val_acc did not improve from 0.49815\n",
            "Epoch 308/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 9.7462 - acc: 0.3182 - val_loss: 6.0040 - val_acc: 0.4444\n",
            "\n",
            "Epoch 00308: val_acc did not improve from 0.49815\n",
            "Epoch 309/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 9.7438 - acc: 0.3131 - val_loss: 5.3763 - val_acc: 0.4593\n",
            "\n",
            "Epoch 00309: val_acc did not improve from 0.49815\n",
            "Epoch 310/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 9.9077 - acc: 0.3157 - val_loss: 5.2817 - val_acc: 0.4469\n",
            "\n",
            "Epoch 00310: val_acc did not improve from 0.49815\n",
            "Epoch 311/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 9.6946 - acc: 0.3236 - val_loss: 5.0323 - val_acc: 0.4284\n",
            "\n",
            "Epoch 00311: val_acc did not improve from 0.49815\n",
            "Epoch 312/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 9.5629 - acc: 0.3150 - val_loss: 6.2470 - val_acc: 0.4611\n",
            "\n",
            "Epoch 00312: val_acc did not improve from 0.49815\n",
            "Epoch 313/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 9.7214 - acc: 0.3231 - val_loss: 5.0919 - val_acc: 0.4401\n",
            "\n",
            "Epoch 00313: val_acc did not improve from 0.49815\n",
            "Epoch 314/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 9.6229 - acc: 0.3156 - val_loss: 5.1536 - val_acc: 0.4580\n",
            "\n",
            "Epoch 00314: val_acc did not improve from 0.49815\n",
            "Epoch 315/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.6001 - acc: 0.3162 - val_loss: 5.2650 - val_acc: 0.4716\n",
            "\n",
            "Epoch 00315: val_acc did not improve from 0.49815\n",
            "Epoch 316/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.6343 - acc: 0.3168 - val_loss: 6.3606 - val_acc: 0.4216\n",
            "\n",
            "Epoch 00316: val_acc did not improve from 0.49815\n",
            "Epoch 317/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.6956 - acc: 0.3096 - val_loss: 5.1918 - val_acc: 0.4210\n",
            "\n",
            "Epoch 00317: val_acc did not improve from 0.49815\n",
            "Epoch 318/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.7545 - acc: 0.3088 - val_loss: 5.2821 - val_acc: 0.4488\n",
            "\n",
            "Epoch 00318: val_acc did not improve from 0.49815\n",
            "Epoch 319/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.6013 - acc: 0.3235 - val_loss: 5.6435 - val_acc: 0.4136\n",
            "\n",
            "Epoch 00319: val_acc did not improve from 0.49815\n",
            "Epoch 320/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.5957 - acc: 0.3194 - val_loss: 5.1882 - val_acc: 0.4660\n",
            "\n",
            "Epoch 00320: val_acc did not improve from 0.49815\n",
            "Epoch 321/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.6271 - acc: 0.3093 - val_loss: 6.3079 - val_acc: 0.4296\n",
            "\n",
            "Epoch 00321: val_acc did not improve from 0.49815\n",
            "Epoch 322/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.4418 - acc: 0.3269 - val_loss: 5.1590 - val_acc: 0.4407\n",
            "\n",
            "Epoch 00322: val_acc did not improve from 0.49815\n",
            "Epoch 323/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.5590 - acc: 0.3262 - val_loss: 5.3335 - val_acc: 0.4988\n",
            "\n",
            "Epoch 00323: val_acc improved from 0.49815 to 0.49877, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 324/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 9.5554 - acc: 0.3184 - val_loss: 5.8942 - val_acc: 0.4056\n",
            "\n",
            "Epoch 00324: val_acc did not improve from 0.49877\n",
            "Epoch 325/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.7294 - acc: 0.3096 - val_loss: 5.0696 - val_acc: 0.4704\n",
            "\n",
            "Epoch 00325: val_acc did not improve from 0.49877\n",
            "Epoch 326/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.7305 - acc: 0.3261 - val_loss: 5.4626 - val_acc: 0.4401\n",
            "\n",
            "Epoch 00326: val_acc did not improve from 0.49877\n",
            "Epoch 327/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.7436 - acc: 0.3191 - val_loss: 5.0190 - val_acc: 0.4395\n",
            "\n",
            "Epoch 00327: val_acc did not improve from 0.49877\n",
            "Epoch 328/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 9.5317 - acc: 0.3196 - val_loss: 5.1729 - val_acc: 0.4241\n",
            "\n",
            "Epoch 00328: val_acc did not improve from 0.49877\n",
            "Epoch 329/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.5612 - acc: 0.3239 - val_loss: 4.9701 - val_acc: 0.4340\n",
            "\n",
            "Epoch 00329: val_acc did not improve from 0.49877\n",
            "Epoch 330/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.5792 - acc: 0.3269 - val_loss: 6.1280 - val_acc: 0.3722\n",
            "\n",
            "Epoch 00330: val_acc did not improve from 0.49877\n",
            "Epoch 331/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.5655 - acc: 0.3173 - val_loss: 5.0527 - val_acc: 0.4617\n",
            "\n",
            "Epoch 00331: val_acc did not improve from 0.49877\n",
            "Epoch 332/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.6007 - acc: 0.3231 - val_loss: 5.0018 - val_acc: 0.4556\n",
            "\n",
            "Epoch 00332: val_acc did not improve from 0.49877\n",
            "Epoch 333/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.6214 - acc: 0.3270 - val_loss: 5.6576 - val_acc: 0.4549\n",
            "\n",
            "Epoch 00333: val_acc did not improve from 0.49877\n",
            "Epoch 334/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.4728 - acc: 0.3151 - val_loss: 4.8577 - val_acc: 0.4327\n",
            "\n",
            "Epoch 00334: val_acc did not improve from 0.49877\n",
            "Epoch 335/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.4106 - acc: 0.3164 - val_loss: 5.2548 - val_acc: 0.4599\n",
            "\n",
            "Epoch 00335: val_acc did not improve from 0.49877\n",
            "Epoch 336/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.4042 - acc: 0.3221 - val_loss: 5.3686 - val_acc: 0.4623\n",
            "\n",
            "Epoch 00336: val_acc did not improve from 0.49877\n",
            "Epoch 337/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.6444 - acc: 0.3233 - val_loss: 5.5243 - val_acc: 0.4593\n",
            "\n",
            "Epoch 00337: val_acc did not improve from 0.49877\n",
            "Epoch 338/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.6145 - acc: 0.3196 - val_loss: 5.1709 - val_acc: 0.4630\n",
            "\n",
            "Epoch 00338: val_acc did not improve from 0.49877\n",
            "Epoch 339/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.7850 - acc: 0.3181 - val_loss: 5.3213 - val_acc: 0.4636\n",
            "\n",
            "Epoch 00339: val_acc did not improve from 0.49877\n",
            "Epoch 340/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.7073 - acc: 0.3159 - val_loss: 5.0158 - val_acc: 0.4414\n",
            "\n",
            "Epoch 00340: val_acc did not improve from 0.49877\n",
            "Epoch 341/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.5133 - acc: 0.3108 - val_loss: 5.1889 - val_acc: 0.4475\n",
            "\n",
            "Epoch 00341: val_acc did not improve from 0.49877\n",
            "Epoch 342/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.4406 - acc: 0.3273 - val_loss: 5.4885 - val_acc: 0.4401\n",
            "\n",
            "Epoch 00342: val_acc did not improve from 0.49877\n",
            "Epoch 343/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.5440 - acc: 0.3245 - val_loss: 4.7047 - val_acc: 0.4747\n",
            "\n",
            "Epoch 00343: val_acc did not improve from 0.49877\n",
            "Epoch 344/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.5991 - acc: 0.3105 - val_loss: 5.0297 - val_acc: 0.4352\n",
            "\n",
            "Epoch 00344: val_acc did not improve from 0.49877\n",
            "Epoch 345/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.8105 - acc: 0.3142 - val_loss: 5.2322 - val_acc: 0.4358\n",
            "\n",
            "Epoch 00345: val_acc did not improve from 0.49877\n",
            "Epoch 346/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.5059 - acc: 0.3264 - val_loss: 5.5922 - val_acc: 0.4537\n",
            "\n",
            "Epoch 00346: val_acc did not improve from 0.49877\n",
            "Epoch 347/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.3772 - acc: 0.3176 - val_loss: 5.6017 - val_acc: 0.4562\n",
            "\n",
            "Epoch 00347: val_acc did not improve from 0.49877\n",
            "Epoch 348/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.4201 - acc: 0.3282 - val_loss: 4.8494 - val_acc: 0.4562\n",
            "\n",
            "Epoch 00348: val_acc did not improve from 0.49877\n",
            "Epoch 349/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.3023 - acc: 0.3289 - val_loss: 4.9093 - val_acc: 0.4679\n",
            "\n",
            "Epoch 00349: val_acc did not improve from 0.49877\n",
            "Epoch 350/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.4407 - acc: 0.3250 - val_loss: 5.5008 - val_acc: 0.4284\n",
            "\n",
            "Epoch 00350: val_acc did not improve from 0.49877\n",
            "Epoch 351/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.2912 - acc: 0.3208 - val_loss: 5.2500 - val_acc: 0.4284\n",
            "\n",
            "Epoch 00351: val_acc did not improve from 0.49877\n",
            "Epoch 352/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.5046 - acc: 0.3162 - val_loss: 5.4075 - val_acc: 0.4500\n",
            "\n",
            "Epoch 00352: val_acc did not improve from 0.49877\n",
            "Epoch 353/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.3312 - acc: 0.3267 - val_loss: 5.0736 - val_acc: 0.4741\n",
            "\n",
            "Epoch 00353: val_acc did not improve from 0.49877\n",
            "Epoch 354/500\n",
            "6480/6480 [==============================] - 2s 242us/step - loss: 9.5593 - acc: 0.3196 - val_loss: 5.4257 - val_acc: 0.4772\n",
            "\n",
            "Epoch 00354: val_acc did not improve from 0.49877\n",
            "Epoch 355/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.5021 - acc: 0.3239 - val_loss: 5.2007 - val_acc: 0.4414\n",
            "\n",
            "Epoch 00355: val_acc did not improve from 0.49877\n",
            "Epoch 356/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.6075 - acc: 0.3219 - val_loss: 5.1325 - val_acc: 0.4278\n",
            "\n",
            "Epoch 00356: val_acc did not improve from 0.49877\n",
            "Epoch 357/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 9.3615 - acc: 0.3276 - val_loss: 5.5529 - val_acc: 0.4488\n",
            "\n",
            "Epoch 00357: val_acc did not improve from 0.49877\n",
            "Epoch 358/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 9.3941 - acc: 0.3306 - val_loss: 5.3178 - val_acc: 0.4284\n",
            "\n",
            "Epoch 00358: val_acc did not improve from 0.49877\n",
            "Epoch 359/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 9.2561 - acc: 0.3293 - val_loss: 5.0127 - val_acc: 0.4204\n",
            "\n",
            "Epoch 00359: val_acc did not improve from 0.49877\n",
            "Epoch 360/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 9.4971 - acc: 0.3215 - val_loss: 4.9922 - val_acc: 0.4531\n",
            "\n",
            "Epoch 00360: val_acc did not improve from 0.49877\n",
            "Epoch 361/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 9.4930 - acc: 0.3323 - val_loss: 5.1678 - val_acc: 0.4753\n",
            "\n",
            "Epoch 00361: val_acc did not improve from 0.49877\n",
            "Epoch 362/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 9.3436 - acc: 0.3269 - val_loss: 5.0793 - val_acc: 0.4346\n",
            "\n",
            "Epoch 00362: val_acc did not improve from 0.49877\n",
            "Epoch 363/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.2656 - acc: 0.3216 - val_loss: 5.1232 - val_acc: 0.4753\n",
            "\n",
            "Epoch 00363: val_acc did not improve from 0.49877\n",
            "Epoch 364/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.2280 - acc: 0.3252 - val_loss: 5.0939 - val_acc: 0.4722\n",
            "\n",
            "Epoch 00364: val_acc did not improve from 0.49877\n",
            "Epoch 365/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.1204 - acc: 0.3272 - val_loss: 5.0892 - val_acc: 0.4698\n",
            "\n",
            "Epoch 00365: val_acc did not improve from 0.49877\n",
            "Epoch 366/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.2723 - acc: 0.3276 - val_loss: 5.3276 - val_acc: 0.4123\n",
            "\n",
            "Epoch 00366: val_acc did not improve from 0.49877\n",
            "Epoch 367/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.3292 - acc: 0.3309 - val_loss: 4.9115 - val_acc: 0.4580\n",
            "\n",
            "Epoch 00367: val_acc did not improve from 0.49877\n",
            "Epoch 368/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.4650 - acc: 0.3231 - val_loss: 5.2443 - val_acc: 0.4148\n",
            "\n",
            "Epoch 00368: val_acc did not improve from 0.49877\n",
            "Epoch 369/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.5035 - acc: 0.3179 - val_loss: 5.6853 - val_acc: 0.4679\n",
            "\n",
            "Epoch 00369: val_acc did not improve from 0.49877\n",
            "Epoch 370/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.2616 - acc: 0.3238 - val_loss: 4.9794 - val_acc: 0.4809\n",
            "\n",
            "Epoch 00370: val_acc did not improve from 0.49877\n",
            "Epoch 371/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.1922 - acc: 0.3247 - val_loss: 4.6775 - val_acc: 0.4623\n",
            "\n",
            "Epoch 00371: val_acc did not improve from 0.49877\n",
            "Epoch 372/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.1595 - acc: 0.3194 - val_loss: 5.8002 - val_acc: 0.4753\n",
            "\n",
            "Epoch 00372: val_acc did not improve from 0.49877\n",
            "Epoch 373/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.3742 - acc: 0.3215 - val_loss: 4.8685 - val_acc: 0.4543\n",
            "\n",
            "Epoch 00373: val_acc did not improve from 0.49877\n",
            "Epoch 374/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.4499 - acc: 0.3255 - val_loss: 5.1696 - val_acc: 0.4389\n",
            "\n",
            "Epoch 00374: val_acc did not improve from 0.49877\n",
            "Epoch 375/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.2934 - acc: 0.3244 - val_loss: 5.6014 - val_acc: 0.4389\n",
            "\n",
            "Epoch 00375: val_acc did not improve from 0.49877\n",
            "Epoch 376/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.1454 - acc: 0.3216 - val_loss: 5.0704 - val_acc: 0.4759\n",
            "\n",
            "Epoch 00376: val_acc did not improve from 0.49877\n",
            "Epoch 377/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.2419 - acc: 0.3281 - val_loss: 5.6317 - val_acc: 0.4043\n",
            "\n",
            "Epoch 00377: val_acc did not improve from 0.49877\n",
            "Epoch 378/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.3991 - acc: 0.3316 - val_loss: 6.1646 - val_acc: 0.4025\n",
            "\n",
            "Epoch 00378: val_acc did not improve from 0.49877\n",
            "Epoch 379/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.4095 - acc: 0.3245 - val_loss: 4.7975 - val_acc: 0.4722\n",
            "\n",
            "Epoch 00379: val_acc did not improve from 0.49877\n",
            "Epoch 380/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.4044 - acc: 0.3287 - val_loss: 4.8263 - val_acc: 0.4148\n",
            "\n",
            "Epoch 00380: val_acc did not improve from 0.49877\n",
            "Epoch 381/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.1422 - acc: 0.3184 - val_loss: 5.0364 - val_acc: 0.4685\n",
            "\n",
            "Epoch 00381: val_acc did not improve from 0.49877\n",
            "Epoch 382/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.2610 - acc: 0.3208 - val_loss: 5.0301 - val_acc: 0.4562\n",
            "\n",
            "Epoch 00382: val_acc did not improve from 0.49877\n",
            "Epoch 383/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 9.2346 - acc: 0.3272 - val_loss: 5.3175 - val_acc: 0.4580\n",
            "\n",
            "Epoch 00383: val_acc did not improve from 0.49877\n",
            "Epoch 384/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 9.1629 - acc: 0.3275 - val_loss: 6.3719 - val_acc: 0.4377\n",
            "\n",
            "Epoch 00384: val_acc did not improve from 0.49877\n",
            "Epoch 385/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 9.4572 - acc: 0.3361 - val_loss: 5.1359 - val_acc: 0.4370\n",
            "\n",
            "Epoch 00385: val_acc did not improve from 0.49877\n",
            "Epoch 386/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 9.2742 - acc: 0.3335 - val_loss: 5.1275 - val_acc: 0.4457\n",
            "\n",
            "Epoch 00386: val_acc did not improve from 0.49877\n",
            "Epoch 387/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 9.2615 - acc: 0.3321 - val_loss: 5.3174 - val_acc: 0.4340\n",
            "\n",
            "Epoch 00387: val_acc did not improve from 0.49877\n",
            "Epoch 388/500\n",
            "6480/6480 [==============================] - 2s 258us/step - loss: 9.1105 - acc: 0.3290 - val_loss: 5.0459 - val_acc: 0.4506\n",
            "\n",
            "Epoch 00388: val_acc did not improve from 0.49877\n",
            "Epoch 389/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.1796 - acc: 0.3235 - val_loss: 4.8583 - val_acc: 0.4333\n",
            "\n",
            "Epoch 00389: val_acc did not improve from 0.49877\n",
            "Epoch 390/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 9.1162 - acc: 0.3239 - val_loss: 4.7198 - val_acc: 0.4660\n",
            "\n",
            "Epoch 00390: val_acc did not improve from 0.49877\n",
            "Epoch 391/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.0594 - acc: 0.3394 - val_loss: 6.0948 - val_acc: 0.4537\n",
            "\n",
            "Epoch 00391: val_acc did not improve from 0.49877\n",
            "Epoch 392/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.2837 - acc: 0.3298 - val_loss: 5.1694 - val_acc: 0.4630\n",
            "\n",
            "Epoch 00392: val_acc did not improve from 0.49877\n",
            "Epoch 393/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.1479 - acc: 0.3228 - val_loss: 4.9537 - val_acc: 0.4778\n",
            "\n",
            "Epoch 00393: val_acc did not improve from 0.49877\n",
            "Epoch 394/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.1948 - acc: 0.3327 - val_loss: 5.2433 - val_acc: 0.4099\n",
            "\n",
            "Epoch 00394: val_acc did not improve from 0.49877\n",
            "Epoch 395/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.2533 - acc: 0.3239 - val_loss: 5.2356 - val_acc: 0.4235\n",
            "\n",
            "Epoch 00395: val_acc did not improve from 0.49877\n",
            "Epoch 396/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.1230 - acc: 0.3299 - val_loss: 4.7897 - val_acc: 0.4599\n",
            "\n",
            "Epoch 00396: val_acc did not improve from 0.49877\n",
            "Epoch 397/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 9.1321 - acc: 0.3329 - val_loss: 4.9767 - val_acc: 0.4296\n",
            "\n",
            "Epoch 00397: val_acc did not improve from 0.49877\n",
            "Epoch 398/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.2243 - acc: 0.3142 - val_loss: 4.8621 - val_acc: 0.4722\n",
            "\n",
            "Epoch 00398: val_acc did not improve from 0.49877\n",
            "Epoch 399/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 9.1364 - acc: 0.3247 - val_loss: 4.8525 - val_acc: 0.4426\n",
            "\n",
            "Epoch 00399: val_acc did not improve from 0.49877\n",
            "Epoch 400/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.0988 - acc: 0.3302 - val_loss: 4.8372 - val_acc: 0.4512\n",
            "\n",
            "Epoch 00400: val_acc did not improve from 0.49877\n",
            "Epoch 401/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.1079 - acc: 0.3307 - val_loss: 5.3559 - val_acc: 0.4444\n",
            "\n",
            "Epoch 00401: val_acc did not improve from 0.49877\n",
            "Epoch 402/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.1300 - acc: 0.3267 - val_loss: 5.2131 - val_acc: 0.4877\n",
            "\n",
            "Epoch 00402: val_acc did not improve from 0.49877\n",
            "Epoch 403/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.1235 - acc: 0.3312 - val_loss: 4.8521 - val_acc: 0.4278\n",
            "\n",
            "Epoch 00403: val_acc did not improve from 0.49877\n",
            "Epoch 404/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.0912 - acc: 0.3250 - val_loss: 5.4363 - val_acc: 0.4864\n",
            "\n",
            "Epoch 00404: val_acc did not improve from 0.49877\n",
            "Epoch 405/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.0430 - acc: 0.3330 - val_loss: 4.8255 - val_acc: 0.4438\n",
            "\n",
            "Epoch 00405: val_acc did not improve from 0.49877\n",
            "Epoch 406/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.1744 - acc: 0.3261 - val_loss: 4.8768 - val_acc: 0.4519\n",
            "\n",
            "Epoch 00406: val_acc did not improve from 0.49877\n",
            "Epoch 407/500\n",
            "6480/6480 [==============================] - 2s 252us/step - loss: 9.0106 - acc: 0.3318 - val_loss: 4.9145 - val_acc: 0.4185\n",
            "\n",
            "Epoch 00407: val_acc did not improve from 0.49877\n",
            "Epoch 408/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 9.0844 - acc: 0.3285 - val_loss: 5.2526 - val_acc: 0.4543\n",
            "\n",
            "Epoch 00408: val_acc did not improve from 0.49877\n",
            "Epoch 409/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 9.1705 - acc: 0.3265 - val_loss: 5.3851 - val_acc: 0.4142\n",
            "\n",
            "Epoch 00409: val_acc did not improve from 0.49877\n",
            "Epoch 410/500\n",
            "6480/6480 [==============================] - 2s 256us/step - loss: 9.1768 - acc: 0.3338 - val_loss: 5.6176 - val_acc: 0.4537\n",
            "\n",
            "Epoch 00410: val_acc did not improve from 0.49877\n",
            "Epoch 411/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 9.0090 - acc: 0.3366 - val_loss: 4.7660 - val_acc: 0.4593\n",
            "\n",
            "Epoch 00411: val_acc did not improve from 0.49877\n",
            "Epoch 412/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 9.0247 - acc: 0.3231 - val_loss: 5.0272 - val_acc: 0.4735\n",
            "\n",
            "Epoch 00412: val_acc did not improve from 0.49877\n",
            "Epoch 413/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.0968 - acc: 0.3156 - val_loss: 5.0907 - val_acc: 0.4975\n",
            "\n",
            "Epoch 00413: val_acc did not improve from 0.49877\n",
            "Epoch 414/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.0050 - acc: 0.3377 - val_loss: 4.7644 - val_acc: 0.4599\n",
            "\n",
            "Epoch 00414: val_acc did not improve from 0.49877\n",
            "Epoch 415/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.0948 - acc: 0.3346 - val_loss: 4.5945 - val_acc: 0.4790\n",
            "\n",
            "Epoch 00415: val_acc did not improve from 0.49877\n",
            "Epoch 416/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.0993 - acc: 0.3269 - val_loss: 4.5053 - val_acc: 0.4377\n",
            "\n",
            "Epoch 00416: val_acc did not improve from 0.49877\n",
            "Epoch 417/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.1095 - acc: 0.3336 - val_loss: 5.5132 - val_acc: 0.4389\n",
            "\n",
            "Epoch 00417: val_acc did not improve from 0.49877\n",
            "Epoch 418/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.0291 - acc: 0.3302 - val_loss: 4.8871 - val_acc: 0.4290\n",
            "\n",
            "Epoch 00418: val_acc did not improve from 0.49877\n",
            "Epoch 419/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.2776 - acc: 0.3312 - val_loss: 4.6503 - val_acc: 0.4593\n",
            "\n",
            "Epoch 00419: val_acc did not improve from 0.49877\n",
            "Epoch 420/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 9.0761 - acc: 0.3279 - val_loss: 5.0685 - val_acc: 0.4660\n",
            "\n",
            "Epoch 00420: val_acc did not improve from 0.49877\n",
            "Epoch 421/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 8.9653 - acc: 0.3265 - val_loss: 4.7944 - val_acc: 0.4648\n",
            "\n",
            "Epoch 00421: val_acc did not improve from 0.49877\n",
            "Epoch 422/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.0219 - acc: 0.3219 - val_loss: 4.5030 - val_acc: 0.4605\n",
            "\n",
            "Epoch 00422: val_acc did not improve from 0.49877\n",
            "Epoch 423/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 9.0257 - acc: 0.3375 - val_loss: 7.2777 - val_acc: 0.4426\n",
            "\n",
            "Epoch 00423: val_acc did not improve from 0.49877\n",
            "Epoch 424/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.1076 - acc: 0.3309 - val_loss: 5.0103 - val_acc: 0.4549\n",
            "\n",
            "Epoch 00424: val_acc did not improve from 0.49877\n",
            "Epoch 425/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.9477 - acc: 0.3187 - val_loss: 4.5767 - val_acc: 0.4667\n",
            "\n",
            "Epoch 00425: val_acc did not improve from 0.49877\n",
            "Epoch 426/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.0783 - acc: 0.3381 - val_loss: 5.7434 - val_acc: 0.4352\n",
            "\n",
            "Epoch 00426: val_acc did not improve from 0.49877\n",
            "Epoch 427/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.9656 - acc: 0.3346 - val_loss: 4.5205 - val_acc: 0.4623\n",
            "\n",
            "Epoch 00427: val_acc did not improve from 0.49877\n",
            "Epoch 428/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.9003 - acc: 0.3281 - val_loss: 5.3129 - val_acc: 0.4463\n",
            "\n",
            "Epoch 00428: val_acc did not improve from 0.49877\n",
            "Epoch 429/500\n",
            "6480/6480 [==============================] - 2s 242us/step - loss: 8.9572 - acc: 0.3298 - val_loss: 4.5560 - val_acc: 0.4889\n",
            "\n",
            "Epoch 00429: val_acc did not improve from 0.49877\n",
            "Epoch 430/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 8.9517 - acc: 0.3306 - val_loss: 5.5129 - val_acc: 0.4241\n",
            "\n",
            "Epoch 00430: val_acc did not improve from 0.49877\n",
            "Epoch 431/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.9125 - acc: 0.3316 - val_loss: 4.9554 - val_acc: 0.4278\n",
            "\n",
            "Epoch 00431: val_acc did not improve from 0.49877\n",
            "Epoch 432/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.1562 - acc: 0.3324 - val_loss: 4.4911 - val_acc: 0.4852\n",
            "\n",
            "Epoch 00432: val_acc did not improve from 0.49877\n",
            "Epoch 433/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.1194 - acc: 0.3287 - val_loss: 4.4469 - val_acc: 0.4815\n",
            "\n",
            "Epoch 00433: val_acc did not improve from 0.49877\n",
            "Epoch 434/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.9278 - acc: 0.3343 - val_loss: 4.7139 - val_acc: 0.4889\n",
            "\n",
            "Epoch 00434: val_acc did not improve from 0.49877\n",
            "Epoch 435/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 8.8093 - acc: 0.3349 - val_loss: 4.8732 - val_acc: 0.4457\n",
            "\n",
            "Epoch 00435: val_acc did not improve from 0.49877\n",
            "Epoch 436/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 8.8585 - acc: 0.3227 - val_loss: 4.9784 - val_acc: 0.4920\n",
            "\n",
            "Epoch 00436: val_acc did not improve from 0.49877\n",
            "Epoch 437/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.9837 - acc: 0.3292 - val_loss: 4.9699 - val_acc: 0.4407\n",
            "\n",
            "Epoch 00437: val_acc did not improve from 0.49877\n",
            "Epoch 438/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.8882 - acc: 0.3370 - val_loss: 4.6044 - val_acc: 0.4278\n",
            "\n",
            "Epoch 00438: val_acc did not improve from 0.49877\n",
            "Epoch 439/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.9039 - acc: 0.3256 - val_loss: 5.2393 - val_acc: 0.4290\n",
            "\n",
            "Epoch 00439: val_acc did not improve from 0.49877\n",
            "Epoch 440/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 9.0084 - acc: 0.3272 - val_loss: 5.3261 - val_acc: 0.4741\n",
            "\n",
            "Epoch 00440: val_acc did not improve from 0.49877\n",
            "Epoch 441/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.9900 - acc: 0.3310 - val_loss: 5.7310 - val_acc: 0.4574\n",
            "\n",
            "Epoch 00441: val_acc did not improve from 0.49877\n",
            "Epoch 442/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 8.9365 - acc: 0.3276 - val_loss: 4.8380 - val_acc: 0.4457\n",
            "\n",
            "Epoch 00442: val_acc did not improve from 0.49877\n",
            "Epoch 443/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.9341 - acc: 0.3372 - val_loss: 4.6961 - val_acc: 0.4358\n",
            "\n",
            "Epoch 00443: val_acc did not improve from 0.49877\n",
            "Epoch 444/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.9139 - acc: 0.3380 - val_loss: 4.6920 - val_acc: 0.4809\n",
            "\n",
            "Epoch 00444: val_acc did not improve from 0.49877\n",
            "Epoch 445/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.8820 - acc: 0.3296 - val_loss: 5.0624 - val_acc: 0.4500\n",
            "\n",
            "Epoch 00445: val_acc did not improve from 0.49877\n",
            "Epoch 446/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.8373 - acc: 0.3247 - val_loss: 4.6662 - val_acc: 0.4284\n",
            "\n",
            "Epoch 00446: val_acc did not improve from 0.49877\n",
            "Epoch 447/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.8997 - acc: 0.3299 - val_loss: 5.2156 - val_acc: 0.4438\n",
            "\n",
            "Epoch 00447: val_acc did not improve from 0.49877\n",
            "Epoch 448/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 9.0219 - acc: 0.3241 - val_loss: 4.6145 - val_acc: 0.4488\n",
            "\n",
            "Epoch 00448: val_acc did not improve from 0.49877\n",
            "Epoch 449/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.8632 - acc: 0.3403 - val_loss: 4.4843 - val_acc: 0.5012\n",
            "\n",
            "Epoch 00449: val_acc improved from 0.49877 to 0.50123, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 450/500\n",
            "6480/6480 [==============================] - 2s 255us/step - loss: 8.7416 - acc: 0.3381 - val_loss: 4.7082 - val_acc: 0.4938\n",
            "\n",
            "Epoch 00450: val_acc did not improve from 0.50123\n",
            "Epoch 451/500\n",
            "6480/6480 [==============================] - 2s 248us/step - loss: 8.7931 - acc: 0.3327 - val_loss: 4.9541 - val_acc: 0.4327\n",
            "\n",
            "Epoch 00451: val_acc did not improve from 0.50123\n",
            "Epoch 452/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.7547 - acc: 0.3343 - val_loss: 4.6527 - val_acc: 0.4710\n",
            "\n",
            "Epoch 00452: val_acc did not improve from 0.50123\n",
            "Epoch 453/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.8451 - acc: 0.3394 - val_loss: 4.7259 - val_acc: 0.4914\n",
            "\n",
            "Epoch 00453: val_acc did not improve from 0.50123\n",
            "Epoch 454/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 8.7772 - acc: 0.3262 - val_loss: 4.8550 - val_acc: 0.4556\n",
            "\n",
            "Epoch 00454: val_acc did not improve from 0.50123\n",
            "Epoch 455/500\n",
            "6480/6480 [==============================] - 2s 249us/step - loss: 9.1377 - acc: 0.3276 - val_loss: 5.2754 - val_acc: 0.4605\n",
            "\n",
            "Epoch 00455: val_acc did not improve from 0.50123\n",
            "Epoch 456/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 8.7336 - acc: 0.3384 - val_loss: 4.5406 - val_acc: 0.4747\n",
            "\n",
            "Epoch 00456: val_acc did not improve from 0.50123\n",
            "Epoch 457/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 8.8253 - acc: 0.3190 - val_loss: 4.5496 - val_acc: 0.4914\n",
            "\n",
            "Epoch 00457: val_acc did not improve from 0.50123\n",
            "Epoch 458/500\n",
            "6480/6480 [==============================] - 2s 257us/step - loss: 8.7802 - acc: 0.3346 - val_loss: 4.4707 - val_acc: 0.4883\n",
            "\n",
            "Epoch 00458: val_acc did not improve from 0.50123\n",
            "Epoch 459/500\n",
            "6480/6480 [==============================] - 2s 254us/step - loss: 8.7177 - acc: 0.3332 - val_loss: 4.5907 - val_acc: 0.4420\n",
            "\n",
            "Epoch 00459: val_acc did not improve from 0.50123\n",
            "Epoch 460/500\n",
            "6480/6480 [==============================] - 2s 253us/step - loss: 8.8324 - acc: 0.3335 - val_loss: 4.8185 - val_acc: 0.4660\n",
            "\n",
            "Epoch 00460: val_acc did not improve from 0.50123\n",
            "Epoch 461/500\n",
            "6480/6480 [==============================] - 2s 250us/step - loss: 8.9725 - acc: 0.3210 - val_loss: 4.8201 - val_acc: 0.5117\n",
            "\n",
            "Epoch 00461: val_acc improved from 0.50123 to 0.51173, saving model to /content/drive/colab/AUTOENCODER/2019-04-16T21:27:23.289363/weights.best.hdf5\n",
            "Epoch 462/500\n",
            "6480/6480 [==============================] - 2s 258us/step - loss: 8.8092 - acc: 0.3275 - val_loss: 4.5626 - val_acc: 0.4796\n",
            "\n",
            "Epoch 00462: val_acc did not improve from 0.51173\n",
            "Epoch 463/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 8.8036 - acc: 0.3319 - val_loss: 4.8027 - val_acc: 0.4383\n",
            "\n",
            "Epoch 00463: val_acc did not improve from 0.51173\n",
            "Epoch 464/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 8.8519 - acc: 0.3343 - val_loss: 4.6297 - val_acc: 0.4741\n",
            "\n",
            "Epoch 00464: val_acc did not improve from 0.51173\n",
            "Epoch 465/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 8.7950 - acc: 0.3343 - val_loss: 5.0157 - val_acc: 0.4309\n",
            "\n",
            "Epoch 00465: val_acc did not improve from 0.51173\n",
            "Epoch 466/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.7915 - acc: 0.3307 - val_loss: 4.5413 - val_acc: 0.4759\n",
            "\n",
            "Epoch 00466: val_acc did not improve from 0.51173\n",
            "Epoch 467/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.8294 - acc: 0.3302 - val_loss: 4.7754 - val_acc: 0.4710\n",
            "\n",
            "Epoch 00467: val_acc did not improve from 0.51173\n",
            "Epoch 468/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 8.7937 - acc: 0.3451 - val_loss: 5.6022 - val_acc: 0.4660\n",
            "\n",
            "Epoch 00468: val_acc did not improve from 0.51173\n",
            "Epoch 469/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.8505 - acc: 0.3410 - val_loss: 4.7822 - val_acc: 0.4611\n",
            "\n",
            "Epoch 00469: val_acc did not improve from 0.51173\n",
            "Epoch 470/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.9318 - acc: 0.3333 - val_loss: 5.2316 - val_acc: 0.4772\n",
            "\n",
            "Epoch 00470: val_acc did not improve from 0.51173\n",
            "Epoch 471/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 8.8798 - acc: 0.3290 - val_loss: 5.0621 - val_acc: 0.4475\n",
            "\n",
            "Epoch 00471: val_acc did not improve from 0.51173\n",
            "Epoch 472/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 8.6882 - acc: 0.3312 - val_loss: 4.5014 - val_acc: 0.4667\n",
            "\n",
            "Epoch 00472: val_acc did not improve from 0.51173\n",
            "Epoch 473/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.8595 - acc: 0.3255 - val_loss: 4.4408 - val_acc: 0.4525\n",
            "\n",
            "Epoch 00473: val_acc did not improve from 0.51173\n",
            "Epoch 474/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.9391 - acc: 0.3313 - val_loss: 5.0458 - val_acc: 0.4815\n",
            "\n",
            "Epoch 00474: val_acc did not improve from 0.51173\n",
            "Epoch 475/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.6697 - acc: 0.3340 - val_loss: 5.0796 - val_acc: 0.4759\n",
            "\n",
            "Epoch 00475: val_acc did not improve from 0.51173\n",
            "Epoch 476/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.8206 - acc: 0.3329 - val_loss: 4.5964 - val_acc: 0.4383\n",
            "\n",
            "Epoch 00476: val_acc did not improve from 0.51173\n",
            "Epoch 477/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.8051 - acc: 0.3335 - val_loss: 5.0452 - val_acc: 0.4469\n",
            "\n",
            "Epoch 00477: val_acc did not improve from 0.51173\n",
            "Epoch 478/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.8202 - acc: 0.3298 - val_loss: 4.5171 - val_acc: 0.4463\n",
            "\n",
            "Epoch 00478: val_acc did not improve from 0.51173\n",
            "Epoch 479/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.7789 - acc: 0.3295 - val_loss: 5.0266 - val_acc: 0.4926\n",
            "\n",
            "Epoch 00479: val_acc did not improve from 0.51173\n",
            "Epoch 480/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.6796 - acc: 0.3335 - val_loss: 4.2751 - val_acc: 0.4821\n",
            "\n",
            "Epoch 00480: val_acc did not improve from 0.51173\n",
            "Epoch 481/500\n",
            "6480/6480 [==============================] - 2s 247us/step - loss: 8.7020 - acc: 0.3381 - val_loss: 6.0372 - val_acc: 0.4840\n",
            "\n",
            "Epoch 00481: val_acc did not improve from 0.51173\n",
            "Epoch 482/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.7561 - acc: 0.3366 - val_loss: 4.8265 - val_acc: 0.4519\n",
            "\n",
            "Epoch 00482: val_acc did not improve from 0.51173\n",
            "Epoch 483/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 8.7440 - acc: 0.3373 - val_loss: 4.3925 - val_acc: 0.4636\n",
            "\n",
            "Epoch 00483: val_acc did not improve from 0.51173\n",
            "Epoch 484/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 8.8772 - acc: 0.3361 - val_loss: 4.3421 - val_acc: 0.4778\n",
            "\n",
            "Epoch 00484: val_acc did not improve from 0.51173\n",
            "Epoch 485/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.7175 - acc: 0.3344 - val_loss: 4.6550 - val_acc: 0.4858\n",
            "\n",
            "Epoch 00485: val_acc did not improve from 0.51173\n",
            "Epoch 486/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 8.5837 - acc: 0.3400 - val_loss: 5.2516 - val_acc: 0.4809\n",
            "\n",
            "Epoch 00486: val_acc did not improve from 0.51173\n",
            "Epoch 487/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.6518 - acc: 0.3259 - val_loss: 4.9235 - val_acc: 0.4827\n",
            "\n",
            "Epoch 00487: val_acc did not improve from 0.51173\n",
            "Epoch 488/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.7171 - acc: 0.3387 - val_loss: 4.4752 - val_acc: 0.4846\n",
            "\n",
            "Epoch 00488: val_acc did not improve from 0.51173\n",
            "Epoch 489/500\n",
            "6480/6480 [==============================] - 2s 246us/step - loss: 8.5971 - acc: 0.3273 - val_loss: 4.4710 - val_acc: 0.4858\n",
            "\n",
            "Epoch 00489: val_acc did not improve from 0.51173\n",
            "Epoch 490/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.8165 - acc: 0.3372 - val_loss: 4.3620 - val_acc: 0.4630\n",
            "\n",
            "Epoch 00490: val_acc did not improve from 0.51173\n",
            "Epoch 491/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.6514 - acc: 0.3296 - val_loss: 4.7279 - val_acc: 0.4710\n",
            "\n",
            "Epoch 00491: val_acc did not improve from 0.51173\n",
            "Epoch 492/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.6568 - acc: 0.3326 - val_loss: 4.5122 - val_acc: 0.4617\n",
            "\n",
            "Epoch 00492: val_acc did not improve from 0.51173\n",
            "Epoch 493/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.7300 - acc: 0.3309 - val_loss: 4.8349 - val_acc: 0.4549\n",
            "\n",
            "Epoch 00493: val_acc did not improve from 0.51173\n",
            "Epoch 494/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.7980 - acc: 0.3269 - val_loss: 5.1345 - val_acc: 0.4735\n",
            "\n",
            "Epoch 00494: val_acc did not improve from 0.51173\n",
            "Epoch 495/500\n",
            "6480/6480 [==============================] - 2s 242us/step - loss: 8.7910 - acc: 0.3301 - val_loss: 4.3689 - val_acc: 0.4975\n",
            "\n",
            "Epoch 00495: val_acc did not improve from 0.51173\n",
            "Epoch 496/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 8.8607 - acc: 0.3282 - val_loss: 5.1459 - val_acc: 0.4611\n",
            "\n",
            "Epoch 00496: val_acc did not improve from 0.51173\n",
            "Epoch 497/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.7858 - acc: 0.3332 - val_loss: 4.5438 - val_acc: 0.4728\n",
            "\n",
            "Epoch 00497: val_acc did not improve from 0.51173\n",
            "Epoch 498/500\n",
            "6480/6480 [==============================] - 2s 244us/step - loss: 8.8529 - acc: 0.3373 - val_loss: 4.6785 - val_acc: 0.5006\n",
            "\n",
            "Epoch 00498: val_acc did not improve from 0.51173\n",
            "Epoch 499/500\n",
            "6480/6480 [==============================] - 2s 243us/step - loss: 8.6572 - acc: 0.3335 - val_loss: 4.4881 - val_acc: 0.4512\n",
            "\n",
            "Epoch 00499: val_acc did not improve from 0.51173\n",
            "Epoch 500/500\n",
            "6480/6480 [==============================] - 2s 245us/step - loss: 8.6065 - acc: 0.3392 - val_loss: 5.5048 - val_acc: 0.4864\n",
            "\n",
            "Epoch 00500: val_acc did not improve from 0.51173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPPV1LbY_G45",
        "colab_type": "text"
      },
      "source": [
        "# Display Output of embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6fFU1WcGN25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer_name = 'embedding_1'\n",
        "intermediate_layer_model = Model(inputs=autoencoder.input,\n",
        "                                 outputs=autoencoder.get_layer(layer_name).output)\n",
        "intermediate_output = intermediate_layer_model.predict(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW8rlBiCHi8T",
        "colab_type": "code",
        "outputId": "5ac5686a-6e30-4535-9057-423d4f5d2843",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "print(intermediate_output[0][2])\n",
        "print(test[0][2])\n",
        "print(test_data[0][2])\n",
        "print(tk.word_index[test_data[0][2]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.44445866e-01 -8.63007382e-02 -1.29414245e-01 -1.08260408e-01\n",
            "  5.58174215e-02  1.29623160e-01 -4.06452790e-02  1.23723909e-01\n",
            " -1.17009908e-01  1.29321665e-01  4.66580726e-02  3.17389041e-01\n",
            "  1.21774450e-01 -1.37701720e-01  1.84139013e-01 -1.23050034e-01\n",
            " -1.02554090e-01  4.68946956e-02  8.55813175e-02 -6.77267462e-02\n",
            " -6.03331625e-02 -1.90901384e-01 -3.98145169e-01 -1.91551879e-01\n",
            "  1.15138419e-01  2.77352691e-01 -1.08017869e-01 -5.93758598e-02\n",
            "  6.41453862e-02 -8.82828608e-02 -1.36498615e-01 -2.22374082e-01\n",
            " -1.74276814e-01  5.13376445e-02  6.11800775e-02 -1.53807878e-01\n",
            " -1.25798374e-01 -1.83768949e-04  5.63302152e-02  8.88621882e-02\n",
            "  4.84831305e-03  6.34139627e-02  1.25357509e-02  1.52351102e-02\n",
            "  8.62910226e-02 -2.11389102e-02 -1.20583996e-02 -1.26776826e-02\n",
            "  6.07163869e-02  1.52003421e-02  2.51873899e-02  4.59247194e-02\n",
            "  6.88442662e-02 -2.98830047e-02  5.68836033e-02  3.40720564e-02\n",
            " -6.24874830e-02 -2.90034860e-02  9.25903618e-02 -3.42658684e-02\n",
            "  6.64143413e-02 -9.58317704e-03 -2.31619515e-02  7.47866556e-02\n",
            "  6.55140844e-04 -4.23929811e-01  1.13401644e-01 -1.61020398e-01\n",
            "  1.31197236e-02 -1.44549996e-01  1.04512773e-01  1.46432996e-01\n",
            " -2.49883816e-01  9.09570083e-02 -1.12867467e-01  6.12952597e-02\n",
            " -1.24430530e-01 -1.27263486e-01 -1.55920714e-01  1.04765020e-01\n",
            " -1.00865565e-01  2.13585958e-01  8.76938403e-02 -3.79623562e-01\n",
            "  8.15539286e-02 -1.68490291e-01  1.89758465e-01 -8.27233791e-02\n",
            "  2.54027218e-01 -8.62689465e-02 -2.97929823e-01 -1.76960099e-02\n",
            " -8.79917759e-03  6.06612451e-02  1.95131171e-02  8.78780112e-02\n",
            " -6.40942901e-02  7.48810619e-02  1.06842920e-01  5.40742166e-02\n",
            "  9.70152244e-02  9.40734521e-02  6.53930530e-02  2.11385991e-02\n",
            "  1.43795043e-01  8.83695930e-02  1.08478032e-01  8.89407322e-02\n",
            "  1.19459212e-01  1.20902345e-01  1.12366349e-01  2.25037098e-01\n",
            "  1.33119360e-01  6.73930049e-02  8.93668652e-01  1.80343479e-01\n",
            "  2.73208410e-01  1.92174971e-01  1.12455644e-01  1.25439718e-01\n",
            "  5.97095191e-02  4.95980680e-02 -1.22762239e-02  7.23144338e-02\n",
            " -1.16216555e-01  3.21550556e-02  2.09278226e-01 -2.73136526e-01]\n",
            "115.0\n",
            "r\n",
            "115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odv_5F8MwdCe",
        "colab_type": "code",
        "outputId": "12560184-ba84-4c80-9b5c-c29b81efb7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "source": [
        "# autoencoder_train.history['epochs']\n",
        "print(autoencoder_train.history.keys())\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(autoencoder_train.history['acc'])\n",
        "plt.plot(autoencoder_train.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train accurracy', 'Test accuracy'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(autoencoder_train.history['loss'])\n",
        "plt.plot(autoencoder_train.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train loss', 'Test loss'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAFnCAYAAABKGFvpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0NIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIKPgIB0Xqf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder.save('/content/drive/colab/models/autoencoder.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzOzAzL8poZo",
        "colab_type": "text"
      },
      "source": [
        "# Extract Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHLoEN0KoKWz",
        "colab_type": "code",
        "outputId": "efee4c70-50c6-42d6-c03f-8a7fa3d42f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "# Get Encoder\n",
        "encoder_model = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('conv_encoder').output)\n",
        "encoder_model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_layer (InputLayer)     (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 200, 128)          16512     \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 200, 128)          82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 40, 128)           512       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv_encoder (Conv1D)        (None, 40, 128)           82048     \n",
            "=================================================================\n",
            "Total params: 181,120\n",
            "Trainable params: 180,864\n",
            "Non-trainable params: 256\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YihnoJRpsPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save encoder model to the file\n",
        "encoder_model.save('/content/drive/colab/models/encoder_model3.h5')\n",
        "# Because compilation resets the models' weights, save them along with the model and load them after compilation\n",
        "encoder_model.save_weights('/content/drive/colab/models/encoder_model_copy-weights3.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z9vC6YXp5N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Load encoder model from the file\n",
        "# new_encoder_model = load_model('/content/gdrive/My Drive/colab/models/encoder_model.h5')\n",
        "\n",
        "# new_encoder_model.compile(\n",
        "#    optimizer=optimizer, \n",
        "#    loss=loss, \n",
        "#    metrics=['accuracy']\n",
        "# )\n",
        "\n",
        "# new_encoder_model.load_weights('/content/gdrive/My Drive/colab/models/encoder_model_copy-weights.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}