{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, UpSampling1D, BatchNormalization\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Plot statistical data such as Annotated heatmaps\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset contains 2,000 java files where each file has maximum 2,000 chars\n",
    "DATASET = \"/Users/martinholecek/Desktop/Datasets/Small/Dataset_2000\"\n",
    "FILE_MAX_SIZE = 2000  # 2kB\n",
    "INPUT_SIZE = 2000 # 2kB\n",
    "\n",
    "TENSORBOARD_LOGS_PATH = './logs'\n",
    "CHECKPOINT_FOLDER_PATH = './checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "\n",
    "# max number of chars in a file 1,999 => 2,000\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# DATASET = \"/content/gdrive/My Drive/colab/Datasets/Dataset_2000\"\n",
    "# TENSORBOARD_LOGS_PATH = '/content/gdrive/My Drive/colab/logs'\n",
    "# CHECKPOINT_FOLDER_PATH = '/content/gdrive/My Drive/colab/checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizer and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet():\n",
    "    ''' Create alphabet from ASCII character '''\n",
    "    char_dict = {}\n",
    "    for num in range(127):\n",
    "        char_dict[chr(num)] = num + 1\n",
    "    return char_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\x00': 1, '\\x01': 2, '\\x02': 3, '\\x03': 4, '\\x04': 5, '\\x05': 6, '\\x06': 7, '\\x07': 8, '\\x08': 9, '\\t': 10, '\\n': 11, '\\x0b': 12, '\\x0c': 13, '\\r': 14, '\\x0e': 15, '\\x0f': 16, '\\x10': 17, '\\x11': 18, '\\x12': 19, '\\x13': 20, '\\x14': 21, '\\x15': 22, '\\x16': 23, '\\x17': 24, '\\x18': 25, '\\x19': 26, '\\x1a': 27, '\\x1b': 28, '\\x1c': 29, '\\x1d': 30, '\\x1e': 31, '\\x1f': 32, ' ': 33, '!': 34, '\"': 35, '#': 36, '$': 37, '%': 38, '&': 39, \"'\": 40, '(': 41, ')': 42, '*': 43, '+': 44, ',': 45, '-': 46, '.': 47, '/': 48, '0': 49, '1': 50, '2': 51, '3': 52, '4': 53, '5': 54, '6': 55, '7': 56, '8': 57, '9': 58, ':': 59, ';': 60, '<': 61, '=': 62, '>': 63, '?': 64, '@': 65, 'A': 66, 'B': 67, 'C': 68, 'D': 69, 'E': 70, 'F': 71, 'G': 72, 'H': 73, 'I': 74, 'J': 75, 'K': 76, 'L': 77, 'M': 78, 'N': 79, 'O': 80, 'P': 81, 'Q': 82, 'R': 83, 'S': 84, 'T': 85, 'U': 86, 'V': 87, 'W': 88, 'X': 89, 'Y': 90, 'Z': 91, '[': 92, '\\\\': 93, ']': 94, '^': 95, '_': 96, '`': 97, 'a': 98, 'b': 99, 'c': 100, 'd': 101, 'e': 102, 'f': 103, 'g': 104, 'h': 105, 'i': 106, 'j': 107, 'k': 108, 'l': 109, 'm': 110, 'n': 111, 'o': 112, 'p': 113, 'q': 114, 'r': 115, 's': 116, 't': 117, 'u': 118, 'v': 119, 'w': 120, 'x': 121, 'y': 122, 'z': 123, '{': 124, '|': 125, '}': 126, '~': 127}\n"
     ]
    }
   ],
   "source": [
    "# Create Alphabet Vocabulary (Dictonary)\n",
    "char_dict = get_alphabet()\n",
    "print(char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\x00': 1, '\\x01': 2, '\\x02': 3, '\\x03': 4, '\\x04': 5, '\\x05': 6, '\\x06': 7, '\\x07': 8, '\\x08': 9, '\\t': 10, '\\n': 11, '\\x0b': 12, '\\x0c': 13, '\\r': 14, '\\x0e': 15, '\\x0f': 16, '\\x10': 17, '\\x11': 18, '\\x12': 19, '\\x13': 20, '\\x14': 21, '\\x15': 22, '\\x16': 23, '\\x17': 24, '\\x18': 25, '\\x19': 26, '\\x1a': 27, '\\x1b': 28, '\\x1c': 29, '\\x1d': 30, '\\x1e': 31, '\\x1f': 32, ' ': 33, '!': 34, '\"': 35, '#': 36, '$': 37, '%': 38, '&': 39, \"'\": 40, '(': 41, ')': 42, '*': 43, '+': 44, ',': 45, '-': 46, '.': 47, '/': 48, '0': 49, '1': 50, '2': 51, '3': 52, '4': 53, '5': 54, '6': 55, '7': 56, '8': 57, '9': 58, ':': 59, ';': 60, '<': 61, '=': 62, '>': 63, '?': 64, '@': 65, 'A': 66, 'B': 67, 'C': 68, 'D': 69, 'E': 70, 'F': 71, 'G': 72, 'H': 73, 'I': 74, 'J': 75, 'K': 76, 'L': 77, 'M': 78, 'N': 79, 'O': 80, 'P': 81, 'Q': 82, 'R': 83, 'S': 84, 'T': 85, 'U': 86, 'V': 87, 'W': 88, 'X': 89, 'Y': 90, 'Z': 91, '[': 92, '\\\\': 93, ']': 94, '^': 95, '_': 96, '`': 97, 'a': 98, 'b': 99, 'c': 100, 'd': 101, 'e': 102, 'f': 103, 'g': 104, 'h': 105, 'i': 106, 'j': 107, 'k': 108, 'l': 109, 'm': 110, 'n': 111, 'o': 112, 'p': 113, 'q': 114, 'r': 115, 's': 116, 't': 117, 'u': 118, 'v': 119, 'w': 120, 'x': 121, 'y': 122, 'z': 123, '{': 124, '|': 125, '}': 126, '~': 127, 'UNK': 128}\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary and Add it into the tokenizer\n",
    "tk.word_index = char_dict.copy()\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "print(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading single sample\n",
    "def load_data(data):\n",
    "    ''' File to np array'''\n",
    "    for d in data:\n",
    "        with open(d, \"r\") as file:\n",
    "            content = file.read()\n",
    "            sequence = tk.texts_to_sequences(content)\n",
    "            data = pad_sequences(\n",
    "                sequence, maxlen=INPUT_SIZE, padding='post')\n",
    "            data = np.array(data, dtype='int32')\n",
    "#             data = np.array(data, dtype='float32')\n",
    "            X = data\n",
    "            y = data\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data in batches\n",
    "def load_data_gen(filenames, batch_size=64):\n",
    "    while True:\n",
    "        batch_paths = np.random.choice(a = filenames, size = batch_size)\n",
    "        batch_input = []\n",
    "#         batch_output = []\n",
    "        \n",
    "        for filename in batch_paths:\n",
    "            try:\n",
    "                content = open(filename, 'r').read()\n",
    "                sequence = tk.texts_to_sequences(content)\n",
    "                data = pad_sequences(sequence, maxlen=INPUT_SIZE, padding='post')\n",
    "                data = np.array(data, dtype='int32')\n",
    "                batch_input += [data]\n",
    "            except:\n",
    "                continue\n",
    "        batch_x = np.array(batch_input)\n",
    "        batch_y = np.array(batch_input)\n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset (filenames)\n",
    "dataset_data = [os.path.join(DATASET, fn) for fn in os.listdir(\n",
    "    DATASET) if fn.endswith('.java') and not fn.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n",
      "400\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "# Divide dataset into training and testing\n",
    "train_data, test_data = train_test_split(dataset_data, test_size=0.2)\n",
    "train_data, validation_data = train_test_split(train_data, test_size=0.2)\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Generators (Files are to big to fit into the memory)\n",
    "# train_gen = load_data(train_data)\n",
    "# test_gen = load_data(test_data)\n",
    "# validation_gen = load_data(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Generators (Using batches)\n",
    "train_gen = load_data_gen(train_data)\n",
    "test_gen = load_data_gen(test_data)\n",
    "validation_gen = load_data_gen(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data example\n",
    "# data = next(train_gen)\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129, 128)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# One hot array representation\n",
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "for char, i in tk.word_index.items():\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "print(embedding_weights.shape)\n",
    "print(embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = INPUT_SIZE       # Must be same as an argument max_len inside pad_sequence method\n",
    "embedding_size = vocab_size   # vocab size 128\n",
    "optimizer = 'adam'\n",
    "loss = 'mse'\n",
    "dropout_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single sample\n",
    "# steps_per_epoch = len(train_data)\n",
    "# validation_steps = len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training in batches\n",
    "batch_size = 64 # default value in generator\n",
    "steps_per_epoch = math.ceil(len(train_data) / batch_size)\n",
    "validation_steps = math.ceil(len(validation_data) / batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer (shape=(None, 2000))\n",
    "inputs = Input(shape=(input_size,), name='input_layer', dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding layer (input_dim=129, output_dim=128, input_length=2,000, weights=(129, 128))\n",
    "embedding_layer = Embedding(vocab_size + 1, embedding_size, input_length=input_size, weights=[embedding_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "x = embedding_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "#     x = Conv1D(256, 7, activation='relu', padding='same')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "#     x = Conv1D(256, 7, activation='relu', padding='same')(x)\n",
    "#     x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "#     x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "#     x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "#     x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "#     x = Conv1D(256, 3, activation='relu', padding='same', name='conv_encoder')(x)\n",
    "    \n",
    "    x = Conv1D(128, 5, activation='relu', padding=\"same\")(x)\n",
    "    x = MaxPooling1D(pool_size=5)(x)\n",
    "    x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=4, padding=\"same\")(x)\n",
    "    x = Conv1D(128, 5, activation='relu', padding='same', name='conv_encoder')(x)\n",
    "    \n",
    "#     x = Conv1D(1, 3, activation='relu', padding='same')(x)\n",
    "#     x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "#     x = Conv1D(1, 3, activation='relu', padding='same', name='conv_encoder')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x):\n",
    "#     x = Conv1D(1, 3, activation='relu', padding='same')(x)\n",
    "#     x = UpSampling1D(2)(x)\n",
    "#     x = Conv1D(1, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "    x = UpSampling1D(4)(x)\n",
    "    x = Conv1D(128, 5, activation='relu', padding='same')(x)\n",
    "    x = UpSampling1D(5)(x)\n",
    "    x = Conv1D(1, 5, activation='relu', padding='same')(x)\n",
    "    \n",
    "#     x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "#     x = UpSampling1D(2)(x)\n",
    "#     x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "#     x = UpSampling1D(2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "#     x = UpSampling1D(2)(x)\n",
    "#     x = Conv1D(256, 7, activation='relu', padding='same')(x)\n",
    "#     x = UpSampling1D(2)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Conv1D(1, 7, activation='relu', padding='same')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encoder(x)\n",
    "x = decoder(x)\n",
    "x = Flatten()(x)\n",
    "# x = Dense(2000, activation='relu')(x)\n",
    "predictions = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile model\n",
    "autoencoder = Model(inputs=inputs, outputs=predictions)\n",
    "# opt = adam, loss = mse\n",
    "autoencoder.compile(loss=loss, optimizer = optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model using graph (Save it as png file)\n",
    "# plot_model(autoencoder, to_file='autoencoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 2000, 128)         16512     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2000, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 400, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 400, 128)          82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv_encoder (Conv1D)        (None, 100, 128)          82048     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 100, 128)          82048     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 400, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 400, 128)          82048     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1 (None, 2000, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 2000, 1)           641       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2000)              0         \n",
      "=================================================================\n",
      "Total params: 427,393\n",
      "Trainable params: 427,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print summary of the model\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print weigths and optimizer - Just for testing\n",
    "# autoencoder.get_weights() \n",
    "# autoencoder.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensorboard = TensorBoard(log_dir=TENSORBOARD_LOGS_PATH, \n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=False)\n",
    "# Run Tensorboard\n",
    "# tensorboard --logdir=/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=CHECKPOINT_FOLDER_PATH, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=True, \n",
    "                             period=1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load the latest checkpoint created\n",
    "# latest_checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "# latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           min_delta=0.01, \n",
    "                           patience=5, # Num of epochs with no improvement after which training stops\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_layer to have shape (2000,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e62154d7da01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#                     use_multiprocessing=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#                     callbacks=[tensorboard, checkpoint, early_stop],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_layer to have shape (2000,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "autoencoder_train = autoencoder.fit_generator(train_gen, \n",
    "                    validation_data=validation_gen, \n",
    "                    validation_steps=validation_steps,\n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=1,\n",
    "#                     use_multiprocessing=True,\n",
    "#                     callbacks=[tensorboard, checkpoint, early_stop],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performace Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = autoencoder_train.history['loss']\n",
    "# val_loss = autoencoder_train.history['val_loss']\n",
    "# epochs = range(200)\n",
    "# plt.figure()\n",
    "# plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get Encoder\n",
    "encoder_model = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('conv_encoder').output)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoder model to the file\n",
    "encoder_model.save('models/encoder_model.h5')\n",
    "# Because compilation resets the models' weights, save them along with the model and load them after compilation\n",
    "encoder_model.save_weights('models/encoder_model_copy-weights.h5')\n",
    "\n",
    "# Load encoder model from the file\n",
    "new_encoder_model = load_model('models/encoder_model.h5')\n",
    "\n",
    "new_encoder_model.compile(\n",
    "   optimizer=optimizer, \n",
    "   loss=loss, \n",
    "   metrics=['accuracy']\n",
    ")\n",
    "\n",
    "new_encoder_model.load_weights('models/encoder_model_copy-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make encoder parameters untrainable (parameters will not change while training)\n",
    "for layer in new_encoder_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_encoder_model.summary()\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
