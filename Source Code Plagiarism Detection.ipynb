{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, UpSampling1D, BatchNormalization\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Plot statistical data such as Annotated heatmaps\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from pprint import pprint\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset contains 2,000 java files where each file has maximum 2,000 chars\n",
    "DATASET = \"/Users/martinholecek/Desktop/Datasets/Small/Dataset_2000\"\n",
    "FILE_MAX_SIZE = 2000  # 2kB\n",
    "INPUT_SIZE = 2000 # 2kB\n",
    "\n",
    "# max number of chars in a file 1,999 => 2,000\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizer and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet():\n",
    "    ''' Create alphabet from ASCII character '''\n",
    "    char_dict = {}\n",
    "    for num in range(127):\n",
    "        char_dict[chr(num)] = num + 1\n",
    "    return char_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Alphabet Vocabulary (Dictonary)\n",
    "char_dict = get_alphabet()\n",
    "print(char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary and Add it into the tokenizer\n",
    "tk.word_index = char_dict.copy()\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "print(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    ''' File to np array'''\n",
    "    for d in data:\n",
    "        with open(d, \"r\") as file:\n",
    "            content = file.read()\n",
    "            sequence = tk.texts_to_sequences(content)\n",
    "            data = pad_sequences(\n",
    "                sequence, maxlen=INPUT_SIZE, padding='post')\n",
    "            data = np.array(data, dtype='float32')\n",
    "            X = data\n",
    "            y = data\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset (filenames)\n",
    "dataset_data = [os.path.join(DATASET, fn) for fn in os.listdir(\n",
    "    DATASET) if fn.endswith('.java') and not fn.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset into training and testing\n",
    "train_data, test_data = train_test_split(dataset_data, test_size=0.2)\n",
    "train_data, validation_data = train_test_split(train_data, test_size=0.2)\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Generators (Files are to big to fit into the memory)\n",
    "train_gen = load_data(train_data)\n",
    "test_gen = load_data(test_data)\n",
    "validation_gen = load_data(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data example\n",
    "data = next(train_gen)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot array representation\n",
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "for char, i in tk.word_index.items():\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "print(embedding_weights.shape)\n",
    "print(embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = INPUT_SIZE       # Must be same as an argument max_len inside pad_sequence method\n",
    "embedding_size = vocab_size   # vocab size 256\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "dropout_p = 0.5\n",
    "steps_per_epoch = len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Embedding Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "inputs = Input(shape=(input_size,), name='input_layer', dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding layer (input_dim, output_dim, input_length, weights)\n",
    "embedding_layer = Embedding(vocab_size + 1, embedding_size, input_length=input_size, weights=[embedding_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    x = Conv1D(256, 7, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(256, 7, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(256, 3, activation='relu', padding='same', name='conv_encoder')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = encoder(x)\n",
    "# encoder_model = Model(inputs=inputs, outputs=x)\n",
    "# encoder_model.summary()\n",
    "# Verify the model using graph (Save it as png file)\n",
    "# plot_model(encoder_model, to_file='encoder_model.png', show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x):\n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(256, 7, activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(1, 7, activation='relu', padding='same')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = decoder(x)\n",
    "x = Flatten()(x)\n",
    "predictions = Dense(2000, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile model\n",
    "autoencoder = Model(inputs=inputs, outputs=predictions)\n",
    "# autoencoder.compile(optimizer=optimizer, loss=loss,\n",
    "#               metrics=['accuracy'])\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop(),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model using graph (Save it as png file)\n",
    "plot_model(autoencoder, to_file='autoencoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of the model\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print weigths and optimizer - Just for testing\n",
    "autoencoder.get_weights() \n",
    "autoencoder.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir='./logs', \n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          embeddings_freq=100,\n",
    "                          write_images=False)\n",
    "# Run Tensorboard\n",
    "# tensorboard --logdir=/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='checkpoints/weights.{epoch:02d}-{val_loss:.2f}.hdf5', \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             save_weights_only=True, \n",
    "                            #period=5\n",
    "                            ) # Number of epochs when to save (after 5 epochs, save model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# We can load the latest checkpoint created\n",
    "latest_checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           min_delta=0.01, \n",
    "                           patience=5, # Num of epochs with no improvement after which training stops\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_train = autoencoder.fit_generator(train_gen, \n",
    "                    validation_data=validation_gen, \n",
    "                    validation_steps=len(validation_data),\n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=1, \n",
    "                    verbose=1,\n",
    "                    callbacks=[tensorboard, checkpoint, early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performace Visualization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loss = autoencoder_train.history['loss']\n",
    "val_loss = autoencoder_train.history['val_loss']\n",
    "epochs = range(200)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get Encoder\n",
    "encoder_model = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('conv_encoder').output)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoder model to the file\n",
    "encoder_model.save('models/encoder_model.h5')\n",
    "# Because compilation resets the models' weights, save them along with the model and load them after compilation\n",
    "encoder_model.save_weights('models/encoder_model_copy-weights.h5')\n",
    "\n",
    "# Load encoder model from the file\n",
    "new_encoder_model = load_model('models/encoder_model.h5')\n",
    "\n",
    "new_encoder_model.compile(\n",
    "   optimizer=optimizer, \n",
    "   loss=loss, \n",
    "   metrics=['accuracy']\n",
    ")\n",
    "\n",
    "new_encoder_model.load_weights('models/encoder_model_copy-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make encoder parameters untrainable (parameters will not change while training)\n",
    "for layer in new_encoder_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_encoder_model.summary()\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
