{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns # Plot statistical data such as Annotated heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1 contains 20,000 java files\n",
    "DATASET = \"/Users/martinholecek/Desktop/Datasets/Dataset 1 (20 000)\"\n",
    "FILE_MAX_SIZE = int(2e5)  # 200,000 (200KB)\n",
    "INPUT_SIZE = int(2e5)\n",
    "\n",
    "# max number of chars in a file 192,537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet():\n",
    "    ''' Create alphabet from ASCII character '''\n",
    "    char_dict = {}\n",
    "    for num in range(127):\n",
    "        char_dict[chr(num)] = num + 1\n",
    "    return char_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet():\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    char_dict = {}\n",
    "    for i, char in enumerate(alphabet):\n",
    "        char_dict[char] = i + 1\n",
    "    return char_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\x00': 1, '\\x01': 2, '\\x02': 3, '\\x03': 4, '\\x04': 5, '\\x05': 6, '\\x06': 7, '\\x07': 8, '\\x08': 9, '\\t': 10, '\\n': 11, '\\x0b': 12, '\\x0c': 13, '\\r': 14, '\\x0e': 15, '\\x0f': 16, '\\x10': 17, '\\x11': 18, '\\x12': 19, '\\x13': 20, '\\x14': 21, '\\x15': 22, '\\x16': 23, '\\x17': 24, '\\x18': 25, '\\x19': 26, '\\x1a': 27, '\\x1b': 28, '\\x1c': 29, '\\x1d': 30, '\\x1e': 31, '\\x1f': 32, ' ': 33, '!': 34, '\"': 35, '#': 36, '$': 37, '%': 38, '&': 39, \"'\": 40, '(': 41, ')': 42, '*': 43, '+': 44, ',': 45, '-': 46, '.': 47, '/': 48, '0': 49, '1': 50, '2': 51, '3': 52, '4': 53, '5': 54, '6': 55, '7': 56, '8': 57, '9': 58, ':': 59, ';': 60, '<': 61, '=': 62, '>': 63, '?': 64, '@': 65, 'A': 66, 'B': 67, 'C': 68, 'D': 69, 'E': 70, 'F': 71, 'G': 72, 'H': 73, 'I': 74, 'J': 75, 'K': 76, 'L': 77, 'M': 78, 'N': 79, 'O': 80, 'P': 81, 'Q': 82, 'R': 83, 'S': 84, 'T': 85, 'U': 86, 'V': 87, 'W': 88, 'X': 89, 'Y': 90, 'Z': 91, '[': 92, '\\\\': 93, ']': 94, '^': 95, '_': 96, '`': 97, 'a': 98, 'b': 99, 'c': 100, 'd': 101, 'e': 102, 'f': 103, 'g': 104, 'h': 105, 'i': 106, 'j': 107, 'k': 108, 'l': 109, 'm': 110, 'n': 111, 'o': 112, 'p': 113, 'q': 114, 'r': 115, 's': 116, 't': 117, 'u': 118, 'v': 119, 'w': 120, 'x': 121, 'y': 122, 'z': 123, '{': 124, '|': 125, '}': 126, '~': 127}\n"
     ]
    }
   ],
   "source": [
    "# Create Alphabet Vocabulary (Dictonary)\n",
    "char_dict = get_alphabet()\n",
    "print(char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\x00': 1, '\\x01': 2, '\\x02': 3, '\\x03': 4, '\\x04': 5, '\\x05': 6, '\\x06': 7, '\\x07': 8, '\\x08': 9, '\\t': 10, '\\n': 11, '\\x0b': 12, '\\x0c': 13, '\\r': 14, '\\x0e': 15, '\\x0f': 16, '\\x10': 17, '\\x11': 18, '\\x12': 19, '\\x13': 20, '\\x14': 21, '\\x15': 22, '\\x16': 23, '\\x17': 24, '\\x18': 25, '\\x19': 26, '\\x1a': 27, '\\x1b': 28, '\\x1c': 29, '\\x1d': 30, '\\x1e': 31, '\\x1f': 32, ' ': 33, '!': 34, '\"': 35, '#': 36, '$': 37, '%': 38, '&': 39, \"'\": 40, '(': 41, ')': 42, '*': 43, '+': 44, ',': 45, '-': 46, '.': 47, '/': 48, '0': 49, '1': 50, '2': 51, '3': 52, '4': 53, '5': 54, '6': 55, '7': 56, '8': 57, '9': 58, ':': 59, ';': 60, '<': 61, '=': 62, '>': 63, '?': 64, '@': 65, 'A': 66, 'B': 67, 'C': 68, 'D': 69, 'E': 70, 'F': 71, 'G': 72, 'H': 73, 'I': 74, 'J': 75, 'K': 76, 'L': 77, 'M': 78, 'N': 79, 'O': 80, 'P': 81, 'Q': 82, 'R': 83, 'S': 84, 'T': 85, 'U': 86, 'V': 87, 'W': 88, 'X': 89, 'Y': 90, 'Z': 91, '[': 92, '\\\\': 93, ']': 94, '^': 95, '_': 96, '`': 97, 'a': 98, 'b': 99, 'c': 100, 'd': 101, 'e': 102, 'f': 103, 'g': 104, 'h': 105, 'i': 106, 'j': 107, 'k': 108, 'l': 109, 'm': 110, 'n': 111, 'o': 112, 'p': 113, 'q': 114, 'r': 115, 's': 116, 't': 117, 'u': 118, 'v': 119, 'w': 120, 'x': 121, 'y': 122, 'z': 123, '{': 124, '|': 125, '}': 126, '~': 127, 'UNK': 128}\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary and Add it into the tokenizer\n",
    "tk.word_index = char_dict.copy()\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "print(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    ''' File to np array'''\n",
    "    for d in data:\n",
    "        with open(d, \"r\") as file:\n",
    "            content = file.read()\n",
    "            train_sequence = tk.texts_to_sequences(content)\n",
    "            train_data = pad_sequences(\n",
    "                train_sequence, maxlen=INPUT_SIZE, padding='post')\n",
    "            train_data = np.array(train_data, dtype='float32')\n",
    "            yield train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset (filenames)\n",
    "dataset_data = [os.path.join(DATASET, fn) for fn in os.listdir(\n",
    "    DATASET) if fn.endswith('.java') and not fn.startswith('.')]\n",
    "\n",
    "# Divide dataset into training and testing\n",
    "train_data, test_data = train_test_split(dataset_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Generators (Files are to big to fit into the memory)\n",
    "train_gen = load_data(train_data)\n",
    "test_gen = load_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 48.   0.   0. ...   0.   0.   0.]\n",
      " [ 43.   0.   0. ...   0.   0.   0.]\n",
      " [ 11.   0.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [ 11.   0.   0. ...   0.   0.   0.]\n",
      " [126.   0.   0. ...   0.   0.   0.]\n",
      " [ 11.   0.   0. ...   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# Data example\n",
    "data = next(train_gen)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129, 128)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# One hot array representation\n",
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "for char, i in tk.word_index.items():\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "print(embedding_weights.shape)\n",
    "print(embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initialize Parameters ####\n",
    "input_size = INPUT_SIZE  # pad_sequences maxlen\n",
    "# vocab size 256\n",
    "embedding_size = vocab_size\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "dropout_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "inputs = Input(shape=(input_size,), name='input_layer', dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding layer (input_dim, output_dim, input_length, weights)\n",
    "embedding_layer = Embedding(vocab_size + 1, embedding_size, input_length=input_size, weights=[embedding_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert autoencoder model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary\n",
    "predictions = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the model using graph (Save it as png file)\n",
    "plot_model(model, to_file='network.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 200000)            0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 200000, 128)       16512     \n",
      "=================================================================\n",
      "Total params: 16,512\n",
      "Trainable params: 16,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
