{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout, UpSampling1D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Plot statistical data such as Annotated heatmaps\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1 contains 20,000 java files\n",
    "DATASET = \"/Users/martinholecek/Desktop/Datasets/Small/Dataset_2000\"\n",
    "FILE_MAX_SIZE = 2000  # 2kB\n",
    "INPUT_SIZE = 2000 # 2kB\n",
    "\n",
    "# max number of chars in a file 1,999 => 2,000\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tokenizer and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet():\n",
    "    ''' Create alphabet from ASCII character '''\n",
    "    char_dict = {}\n",
    "    for num in range(127):\n",
    "        char_dict[chr(num)] = num + 1\n",
    "    return char_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\x00': 1, '\\x01': 2, '\\x02': 3, '\\x03': 4, '\\x04': 5, '\\x05': 6, '\\x06': 7, '\\x07': 8, '\\x08': 9, '\\t': 10, '\\n': 11, '\\x0b': 12, '\\x0c': 13, '\\r': 14, '\\x0e': 15, '\\x0f': 16, '\\x10': 17, '\\x11': 18, '\\x12': 19, '\\x13': 20, '\\x14': 21, '\\x15': 22, '\\x16': 23, '\\x17': 24, '\\x18': 25, '\\x19': 26, '\\x1a': 27, '\\x1b': 28, '\\x1c': 29, '\\x1d': 30, '\\x1e': 31, '\\x1f': 32, ' ': 33, '!': 34, '\"': 35, '#': 36, '$': 37, '%': 38, '&': 39, \"'\": 40, '(': 41, ')': 42, '*': 43, '+': 44, ',': 45, '-': 46, '.': 47, '/': 48, '0': 49, '1': 50, '2': 51, '3': 52, '4': 53, '5': 54, '6': 55, '7': 56, '8': 57, '9': 58, ':': 59, ';': 60, '<': 61, '=': 62, '>': 63, '?': 64, '@': 65, 'A': 66, 'B': 67, 'C': 68, 'D': 69, 'E': 70, 'F': 71, 'G': 72, 'H': 73, 'I': 74, 'J': 75, 'K': 76, 'L': 77, 'M': 78, 'N': 79, 'O': 80, 'P': 81, 'Q': 82, 'R': 83, 'S': 84, 'T': 85, 'U': 86, 'V': 87, 'W': 88, 'X': 89, 'Y': 90, 'Z': 91, '[': 92, '\\\\': 93, ']': 94, '^': 95, '_': 96, '`': 97, 'a': 98, 'b': 99, 'c': 100, 'd': 101, 'e': 102, 'f': 103, 'g': 104, 'h': 105, 'i': 106, 'j': 107, 'k': 108, 'l': 109, 'm': 110, 'n': 111, 'o': 112, 'p': 113, 'q': 114, 'r': 115, 's': 116, 't': 117, 'u': 118, 'v': 119, 'w': 120, 'x': 121, 'y': 122, 'z': 123, '{': 124, '|': 125, '}': 126, '~': 127}\n"
     ]
    }
   ],
   "source": [
    "# Create Alphabet Vocabulary (Dictonary)\n",
    "char_dict = get_alphabet()\n",
    "print(char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\x00': 1, '\\x01': 2, '\\x02': 3, '\\x03': 4, '\\x04': 5, '\\x05': 6, '\\x06': 7, '\\x07': 8, '\\x08': 9, '\\t': 10, '\\n': 11, '\\x0b': 12, '\\x0c': 13, '\\r': 14, '\\x0e': 15, '\\x0f': 16, '\\x10': 17, '\\x11': 18, '\\x12': 19, '\\x13': 20, '\\x14': 21, '\\x15': 22, '\\x16': 23, '\\x17': 24, '\\x18': 25, '\\x19': 26, '\\x1a': 27, '\\x1b': 28, '\\x1c': 29, '\\x1d': 30, '\\x1e': 31, '\\x1f': 32, ' ': 33, '!': 34, '\"': 35, '#': 36, '$': 37, '%': 38, '&': 39, \"'\": 40, '(': 41, ')': 42, '*': 43, '+': 44, ',': 45, '-': 46, '.': 47, '/': 48, '0': 49, '1': 50, '2': 51, '3': 52, '4': 53, '5': 54, '6': 55, '7': 56, '8': 57, '9': 58, ':': 59, ';': 60, '<': 61, '=': 62, '>': 63, '?': 64, '@': 65, 'A': 66, 'B': 67, 'C': 68, 'D': 69, 'E': 70, 'F': 71, 'G': 72, 'H': 73, 'I': 74, 'J': 75, 'K': 76, 'L': 77, 'M': 78, 'N': 79, 'O': 80, 'P': 81, 'Q': 82, 'R': 83, 'S': 84, 'T': 85, 'U': 86, 'V': 87, 'W': 88, 'X': 89, 'Y': 90, 'Z': 91, '[': 92, '\\\\': 93, ']': 94, '^': 95, '_': 96, '`': 97, 'a': 98, 'b': 99, 'c': 100, 'd': 101, 'e': 102, 'f': 103, 'g': 104, 'h': 105, 'i': 106, 'j': 107, 'k': 108, 'l': 109, 'm': 110, 'n': 111, 'o': 112, 'p': 113, 'q': 114, 'r': 115, 's': 116, 't': 117, 'u': 118, 'v': 119, 'w': 120, 'x': 121, 'y': 122, 'z': 123, '{': 124, '|': 125, '}': 126, '~': 127, 'UNK': 128}\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary and Add it into the tokenizer\n",
    "tk.word_index = char_dict.copy()\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "print(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data):\n",
    "    ''' File to np array'''\n",
    "    for d in data:\n",
    "        with open(d, \"r\") as file:\n",
    "            content = file.read()\n",
    "            train_sequence = tk.texts_to_sequences(content)\n",
    "            train_data = pad_sequences(\n",
    "                train_sequence, maxlen=INPUT_SIZE, padding='post')\n",
    "            train_data = np.array(train_data, dtype='float32')\n",
    "            yield train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset (filenames)\n",
    "dataset_data = [os.path.join(DATASET, fn) for fn in os.listdir(\n",
    "    DATASET) if fn.endswith('.java') and not fn.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide dataset into training and testing\n",
    "train_data, test_data = train_test_split(dataset_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Generators (Files are to big to fit into the memory)\n",
    "train_gen = load_data(train_data)\n",
    "test_gen = load_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 65.   0.   0. ...   0.   0.   0.]\n",
      " [100.   0.   0. ...   0.   0.   0.]\n",
      " [112.   0.   0. ...   0.   0.   0.]\n",
      " ...\n",
      " [126.   0.   0. ...   0.   0.   0.]\n",
      " [ 11.   0.   0. ...   0.   0.   0.]\n",
      " [126.   0.   0. ...   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# Data example\n",
    "data = next(train_gen)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129, 128)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# One hot array representation\n",
    "embedding_weights = []\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "for char, i in tk.word_index.items():\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "print(embedding_weights.shape)\n",
    "print(embedding_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Initialize Parameters ####\n",
    "input_size = INPUT_SIZE   # Must be same as an argument max_len inside pad_sequence method\n",
    "# vocab size 256\n",
    "embedding_size = vocab_size\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "dropout_p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "inputs = Input(shape=(input_size,), name='input_layer', dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding layer (input_dim, output_dim, input_length, weights)\n",
    "embedding_layer = Embedding(vocab_size + 1, embedding_size, input_length=input_size, weights=[embedding_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "x = embedding_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x):\n",
    "    x = Conv1D(256, 7, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(256, 7, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPooling1D(pool_size=2, padding=\"same\")(x)\n",
    "    \n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 2000, 128)         16512     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2000, 256)         229632    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2000, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1000, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1000, 256)         459008    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 500, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500, 256)          1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 250, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 250, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 125, 256)          196864    \n",
      "=================================================================\n",
      "Total params: 1,297,792\n",
      "Trainable params: 1,296,768\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x = encoder(x)\n",
    "encoder_model = Model(inputs=inputs, outputs=x)\n",
    "encoder_model.summary()\n",
    "# Verify the model using graph (Save it as png file)\n",
    "plot_model(encoder_model, to_file='encoder_model.png', show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(x):\n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = Conv1D(256, 7, activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = UpSampling1D(2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(256, 7, activation='sigmoid', padding='same')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary\n",
    "predictions = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model using graph (Save it as png file)\n",
    "plot_model(model, to_file='autoencoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 2000, 128)         16512     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2000, 256)         229632    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2000, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1000, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1000, 256)         459008    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 500, 256)          196864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500, 256)          1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 250, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 250, 256)          196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 125, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 125, 256)          196864    \n",
      "_________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1 (None, 250, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 250, 256)          196864    \n",
      "_________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1 (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 500, 256)          1024      \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 500, 256)          196864    \n",
      "_________________________________________________________________\n",
      "up_sampling1d_3 (UpSampling1 (None, 1000, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 1000, 256)         459008    \n",
      "_________________________________________________________________\n",
      "up_sampling1d_4 (UpSampling1 (None, 2000, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 2000, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 2000, 256)         459008    \n",
      "=================================================================\n",
      "Total params: 2,808,448\n",
      "Trainable params: 2,806,400\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print summary of the model\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
